{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "U2-S2-D3-Gradient Descent Assignment.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zarrinan/DS-Unit-2-Sprint-2-Linear-Regression/blob/master/module3-gradient-descent/U2_S2_D3_Gradient_Descent_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "Qhm0Y_jqXKRv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Gradient Descent Implementation Challenge!!\n",
        "\n",
        "## Use gradient descent to find the optimal parameters of a **multiple** regression model. (We only showed an implementation for a bivariate model during lecture.)\n",
        "\n",
        "A note: Implementing gradient descent in any context is not trivial, particularly the step where we calculate the gradient will change based on the number of parameters that we're trying to optimize for. You will need to research what the gradient of a multiple regression model looks like. This challenge is pretty open-ended but I hope it will be thrilling. Please work together, help each other, share resources and generally expand your understanding of gradient descent as you try and achieve this implementation. \n",
        "\n",
        "## Suggestions:\n",
        "\n",
        "Start off with a model that has just two $X$ variables You can use any datasets that have at least two x variables. Potential candidates might be the blood pressure dataset that we used during lecture on Monday: [HERE](https://college.cengage.com/mathematics/brase/understandable_statistics/7e/students/datasets/mlr/excel/mlr02.xls) or any of the housing datasets. You would just need to select from them the two varaibles $x$ variables and one y variable that you want to work with that you most want to work with. \n",
        "\n",
        "Use Sklearn to find the optimal parameters of your model first. (like we did during the lecture.) So that you can compare the parameter estimates of your gradient-descent linear regression to the estimates of OLS linear regression. If implemented correctly they should be nearly identical.\n",
        "\n",
        "Becoming a Data Scientist is all about striking out into the unknown, getting stuck and then researching and fighting and learning until you get yourself unstuck. Work together! And fight to take your own learning-rate fueled step towards your own optimal understanding of gradient descent! \n"
      ]
    },
    {
      "metadata": {
        "id": "eyP_07g27nIm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###OSL"
      ]
    },
    {
      "metadata": {
        "id": "_tWzF6IqXIIq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "##### Make it Hap'n Cap'n #####"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "v_xBAX3N4zsG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install seaborn==0.9.0 -q"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yBv3zaCIiDoP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "2a5d4522-cdbf-4287-ca96-78b68581bdb0"
      },
      "cell_type": "code",
      "source": [
        "!pip install scipy"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from scipy) (1.14.6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "eno5JiUirKXY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#IMPORTS\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy import optimize"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pvoS4kU445YT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sns.__version__"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0tLRJJV-4_bH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "raw = pd.read_csv('https://raw.githubusercontent.com/ryanleeallred/datasets/master/Ames%20Housing%20Data/train.csv', index_col=0)\n",
        "names =['MSSubClass', 'LotArea', 'Neighborhood', 'OverallQual', 'OverallCond',\n",
        "       'YearBuilt', 'YearRemodAdd', 'BsmtFinSF1', 'TotalBsmtSF', '1stFlrSF',\n",
        "       '2ndFlrSF', 'GrLivArea', 'BsmtFullBath', 'FullBath', 'HalfBath',\n",
        "       'BedroomAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageCars',\n",
        "       'GarageArea', 'OpenPorchSF', 'MoSold', 'YrSold', 'SalePrice']\n",
        "raw2 = raw[names].copy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HmxnMFm05ScD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Auto encodes any dataframe column of type category or object.\n",
        "def dummy_encode(df):\n",
        "        columnsToEncode = list(df.select_dtypes(include=['category','object']))\n",
        "        le = LabelEncoder()\n",
        "        for feature in columnsToEncode:\n",
        "            try:\n",
        "                df[feature] = le.fit_transform(df[feature])\n",
        "            except:\n",
        "                print('Error encoding '+feature)\n",
        "        return df\n",
        "df = dummy_encode(raw2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NHc0ZfCG5Tsq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df['ln_price'] = np.log(df['SalePrice'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RfCjSQp95h-K",
        "colab_type": "code",
        "outputId": "cc480c67-fc6d-4459-bacd-76a1e64159d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "Q1 = df.quantile(0.25)\n",
        "Q3 = df.quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "df = df[~((df < (Q1 - 1.5 * IQR)) |(df > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
        "df.shape"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(998, 25)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "metadata": {
        "id": "beJDLCmO6h4D",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df.columns\n",
        "cols = ['OverallQual', 'GrLivArea']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WNxVbh5prM3i",
        "colab_type": "code",
        "outputId": "966d038c-61d6-4604-ba0a-577d08aa3aec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "cell_type": "code",
      "source": [
        "#OSL\n",
        "def print_rse_r2(df, target, cols):\n",
        "  y = df.loc[:, [target]].values\n",
        "  \n",
        "  \n",
        "  X = df.loc[:, cols].values\n",
        "  # Slit into test and train datasets\n",
        "  X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=.25, random_state=42)\n",
        "\n",
        "  # fit model using train datasets\n",
        "  model = LinearRegression()\n",
        "  model.fit(X_train, Y_train)\n",
        "\n",
        "  # Create new predictions using x_test\n",
        "  y_pred = model.predict(X_test)\n",
        "\n",
        "  # Measure Accuracy using y_test and y_pred\n",
        "  RMSE = (np.sqrt(mean_squared_error(Y_test, y_pred)))\n",
        "  R2 = r2_score(Y_test, y_pred)\n",
        "\n",
        "  print('RMSE is {}'.format(RMSE))\n",
        "  print('R^2 is {}'.format(R2))\n",
        "  print(\"coefficients: \", model.coef_[0])\n",
        "  print(\"intercepts: \", model.intercept_)\n",
        "  \n",
        "print_rse_r2(df, 'SalePrice', cols)"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RMSE is 25078.13025338941\n",
            "R^2 is 0.779094318746662\n",
            "coefficients:  [27071.05970445    53.86776355]\n",
            "intercepts:  [-67996.22539912]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nvyiuW2Q7W8J",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Gradient Descent"
      ]
    },
    {
      "metadata": {
        "id": "JL-TeIYq8iPr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<b>Cost</b>\n",
        "\\begin{equation}\n",
        "J(\\theta) = 1/2m \\sum_{i=1}^{m} (h_\\theta(x^{(i)} - y^{(i)})^2 \n",
        "\\end{equation}\n",
        "\n",
        "<b>Gradient</b>\n",
        "\n",
        "\\begin{equation}\n",
        "\\frac{\\partial J(\\theta)}{\\partial \\theta_j} = 1/m\\sum_{i=1}^{m}(h_\\theta(x^{(i)}) - y^{(i)}).X_j^{(i)}\n",
        "\\end{equation}\n",
        "\n",
        "<b>Gradients</b>\n",
        "\\begin{equation}\n",
        "\\theta_j: = \\theta_j -\\alpha . 1/m .\\sum_{i=1}^{m}(h_\\theta(x^{(i)}) - y^{(i)}).X_j^{(i)}\n",
        "\\end{equation}\n"
      ]
    },
    {
      "metadata": {
        "id": "UxaVRkN67UFF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "### Define X and y variables\n",
        "x = df[['OverallQual', 'GrLivArea']].values\n",
        "X = df[['OverallQual', 'GrLivArea']].values\n",
        "# Normalize X\n",
        "X = (X - X.mean()) / X.std()\n",
        "# Matrix version \n",
        "X = np.c_[np.ones((X.shape[0], 1)), X]\n",
        "\n",
        "y = df['SalePrice'][:, np.newaxis]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M1Q9HNO1mcDK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class LinearRegressionUsingGD:\n",
        "    \"\"\"Linear Regression Using Gradient Descent.\n",
        "    Parameters\n",
        "    ----------\n",
        "    eta : float\n",
        "        Learning rate\n",
        "    n_iterations : int\n",
        "        No of passes over the training set\n",
        "    Attributes\n",
        "    ----------\n",
        "    w_ : weights/ after fitting the model [theta]\n",
        "    cost_ : total error of the model after each iteration\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, eta=0.05, n_iterations=100):\n",
        "        self.eta = eta\n",
        "        self.n_iterations = n_iterations\n",
        "\n",
        "    def fit(self, x, y):\n",
        "        \"\"\"Fit the training data\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : array-like, shape = [n_samples, n_features]\n",
        "            Training samples\n",
        "        y : array-like, shape = [n_samples, n_target_values]\n",
        "            Target values\n",
        "        Returns\n",
        "        -------\n",
        "        self : object\n",
        "        \"\"\"\n",
        "\n",
        "        self.cost_ = [] \n",
        "        self.w_ = np.zeros((x.shape[1], 1)) #also, known as theta\n",
        "        m = x.shape[0]\n",
        "\n",
        "        for _ in range(self.n_iterations):\n",
        "            y_pred = self.predict(X)\n",
        "            residual = y_pred - y\n",
        "            cost = np.sum((residual ** 2)) / (2 * m) #sse\n",
        "            self.cost_.append(cost)\n",
        "            \n",
        "            gradient_vector = np.dot(X.T, residual) #𝑔𝑟𝑎𝑑𝑖𝑒𝑛𝑡=(−𝑋∗(𝑒𝑟𝑟𝑜𝑟𝑠))/𝑁\n",
        "            \n",
        "            self.w_ -= (self.eta / m) * gradient_vector\n",
        "\n",
        "             \n",
        "        return self\n",
        "\n",
        "    def predict(self, x):\n",
        "        \"\"\" Predicts the value after the model has been trained.\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : array-like, shape = [n_samples, n_features]\n",
        "            Test samples\n",
        "        Returns\n",
        "        -------\n",
        "        Predicted value\n",
        "        \"\"\"\n",
        "        return np.dot(x, self.w_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NfyoWzqEndu9",
        "colab_type": "code",
        "outputId": "774c3ab0-9f6c-421f-af7c-ad97ca62c045",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "cell_type": "code",
      "source": [
        "gd = LinearRegressionUsingGD(eta=0.05, n_iterations=10000)\n",
        "gd.fit(X,y)\n",
        "gd.w_"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 62727.87412894],\n",
              "       [-41187.09329022],\n",
              "       [ 78329.14455482]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    },
    {
      "metadata": {
        "id": "HqsmgxJfp3r8",
        "colab_type": "code",
        "outputId": "ea390d48-aafa-41a3-b03c-dca39dbd31c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 276
        }
      },
      "cell_type": "code",
      "source": [
        "epochs = np.arange(len(gd.cost_))\n",
        "plt.scatter(x=epochs, y=gd.cost_)\n",
        "plt.show()\n"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEDCAYAAADOc0QpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n4cHLOc0AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f8f272ee828>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "h6bTRYfxmYKW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Using scipy fmin_tnc function"
      ]
    },
    {
      "metadata": {
        "id": "kmO_o2jdlC0P",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def sigmoid(x):\n",
        "    # Activation function used to map any real value between 0 and 1\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def net_input(theta, x):\n",
        "    # Computes the weighted sum of inputs\n",
        "    return np.dot(x, theta)\n",
        "\n",
        "def probability(theta, x):\n",
        "    # Returns the probability after passing through sigmoid\n",
        "    return sigmoid(net_input(theta, x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_PaIoLYhlH1b",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def cost_function(theta, x, y):\n",
        "    # Computes the cost function for all the training samples\n",
        "    m = x.shape[0]\n",
        "    total_cost = -(1 / m) * np.sum(\n",
        "        y * np.log(probability(theta, x)) + (1 - y) * np.log(\n",
        "            1 - probability(theta, x)))\n",
        "    return total_cost\n",
        "\n",
        "def gradient(theta, x, y):\n",
        "    # Computes the gradient of the cost function at the point theta\n",
        "    m = x.shape[0]\n",
        "    return (1 / m) * np.dot(x.T, sigmoid(net_input(theta,   x)) - y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_r65Q7m5lMz-",
        "colab_type": "code",
        "outputId": "fbe52de0-2508-4d71-abfd-d59b1433986a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "def fit( x, y, theta):\n",
        "    opt_weights = optimize.fmin_tnc(func=cost_function, x0=theta,\n",
        "                  fprime=gradient,args=(x, y.flatten()))\n",
        "    return opt_weights[0]\n",
        "parameters = fit(X, y, theta)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: RuntimeWarning: divide by zero encountered in log\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "SzAZN_b8lRHw",
        "colab_type": "code",
        "outputId": "286c7af1-e721-41fd-afe0-5fdb213e98fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "print(parameters)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 6.03199181 -5.60816309  5.67129453]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xu3iA-gSVPIk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Another approach"
      ]
    },
    {
      "metadata": {
        "id": "4ygw-mZOSEep",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def gradient_descent(X, y, iterations, alpha):\n",
        "    \"\"\"\n",
        "    gradient descent regression\n",
        " \n",
        "    X = features, np.array\n",
        "    y = target, np.array\n",
        "    iterations = steps taken to converge\n",
        "    alpha = learning rate\n",
        "    \"\"\"\n",
        "    X_norm = (X - X.mean()) / X.std()\n",
        "    X_linalg = np.c_[np.ones(X_norm.shape[0]), X_norm]\n",
        "    \n",
        "    theta = np.zeros((X_linalg.shape[1], 1)) #initial guess for theta (weights) are zeros\n",
        "    \n",
        "    for _ in range(iterations):\n",
        "        pred = np.dot(X_linalg, theta)\n",
        "        error = pred - y\n",
        "        gradient = np.dot(X_linalg.T, error) / len(y)\n",
        "        theta = theta - alpha*gradient\n",
        "    \n",
        "    return theta, pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iUU2PDNuTbG2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "theta, pred = gradient_descent(x, y, 10000, 0.05)\n",
        "theta"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cQQP2LdRUfYK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# ### Plotting\n",
        "# fig = plt.figure(figsize=(8,8))\n",
        "# ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "# # Plot datapoints\n",
        "# ax.scatter(x[:,0], x[:,1], y, alpha=0.5);\n",
        "# ax.scatter(x[:,0], x[:,1], pred, alpha=0.5, color='purple');\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RCs6EmWhYPM-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Stretch Goals\n",
        "\n",
        "If you happen upon the most useful resources for accomplishing this challenge first, I want you to spend time today studying other variations of Gradient Descent-Based Optimizers.\n",
        "\n",
        "- Try and write a function that can perform gradient descent for arbitarily large (in dimensionality) multiple regression models. \n",
        "- Create a notebook for yourself exploring these topics\n",
        "- How do they differ from the \"vanilla\" gradient descent we explored today\n",
        "- How do these different gradient descent-based optimizers seek to overcome the challenge of finding the global minimum among various local minima?\n",
        "- Write a blog post that reteaches what you have learned about these other gradient descent-based optimizers.\n",
        "\n",
        "[Overview of GD-based optimizers](http://ruder.io/optimizing-gradient-descent/)\n",
        "\n",
        "[Siraj Raval - Evolution of Gradient Descent-Based Optimizers](https://youtu.be/nhqo0u1a6fw)"
      ]
    }
  ]
}