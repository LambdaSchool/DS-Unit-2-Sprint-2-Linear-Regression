{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CTolbert DS223 Gradient Descent Assignment.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hughjafro/DS-Unit-2-Sprint-2-Linear-Regression/blob/master/module3-gradient-descent/CTolbert_DS223_Gradient_Descent_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "Qhm0Y_jqXKRv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Gradient Descent Implementation Challenge!!\n",
        "\n",
        "## Use gradient descent to find the optimal parameters of a **multiple** regression model. (We only showed an implementation for a bivariate model during lecture.)\n",
        "\n",
        "A note: Implementing gradient descent in any context is not trivial, particularly the step where we calculate the gradient will change based on the number of parameters that we're trying to optimize for. You will need to research what the gradient of a multiple regression model looks like. This challenge is pretty open-ended but I hope it will be thrilling. Please work together, help each other, share resources and generally expand your understanding of gradient descent as you try and achieve this implementation. \n",
        "\n",
        "## Suggestions:\n",
        "\n",
        "Start off with a model that has just two $X$ variables You can use any datasets that have at least two x variables. Potential candidates might be the blood pressure dataset that we used during lecture on Monday: [HERE](https://college.cengage.com/mathematics/brase/understandable_statistics/7e/students/datasets/mlr/excel/mlr02.xls) or any of the housing datasets. You would just need to select from them the two varaibles $x$ variables and one y variable that you want to work with that you most want to work with. \n",
        "\n",
        "Use Sklearn to find the optimal parameters of your model first. (like we did during the lecture.) So that you can compare the parameter estimates of your gradient-descent linear regression to the estimates of OLS linear regression. If implemented correctly they should be nearly identical.\n",
        "\n",
        "Becoming a Data Scientist is all about striking out into the unknown, getting stuck and then researching and fighting and learning until you get yourself unstuck. Work together! And fight to take your own learning-rate fueled step towards your own optimal understanding of gradient descent! \n"
      ]
    },
    {
      "metadata": {
        "id": "_tWzF6IqXIIq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "##### Make it Hap'n Cap'n #####\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "wuEYq_tX0mI6",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "\n",
        "# df = pd.read_excel('https://college.cengage.com/mathematics/brase/understandable_statistics/7e/students/datasets/mlr/excel/mlr02.xls')\n",
        "# df = df.rename(index=str, columns={\"X1\": \"y\", \"X2\": \"age\", \"X3\": \"weight\"})\n",
        "# print(df.shape)\n",
        "# df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "S7yHhzcZzV0E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "27547f98-c3d0-4def-83c0-5c709a27367d"
      },
      "cell_type": "code",
      "source": [
        "# Import data\n",
        "df = pd.read_excel('https://github.com/hughjafro/DS-Unit-2-Sprint-2-Linear-Regression/blob/master/module3-gradient-descent/mlr02.xls?raw=true', index=0)\n",
        "\n",
        "df = df.rename(index=str, columns={'X1': 'y', 'X2': 'age', 'X3': 'weight'})\n",
        "print(df.shape)\n",
        "df.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "*** No CODEPAGE record, no encoding_override: will use 'ascii'\n",
            "(11, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>y</th>\n",
              "      <th>age</th>\n",
              "      <th>weight</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>132</td>\n",
              "      <td>52</td>\n",
              "      <td>173</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>143</td>\n",
              "      <td>59</td>\n",
              "      <td>184</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>153</td>\n",
              "      <td>67</td>\n",
              "      <td>194</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>162</td>\n",
              "      <td>73</td>\n",
              "      <td>211</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>154</td>\n",
              "      <td>64</td>\n",
              "      <td>196</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     y  age  weight\n",
              "0  132   52     173\n",
              "1  143   59     184\n",
              "2  153   67     194\n",
              "3  162   73     211\n",
              "4  154   64     196"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "U5vUbR4cIv8k",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Calculate with sklearn first"
      ]
    },
    {
      "metadata": {
        "id": "l5Lg6Vw71Wkj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "a4d8afdd-fbfb-4c09-ba2e-de897d3a74b5"
      },
      "cell_type": "code",
      "source": [
        "# Calculate parameters using sklearn\n",
        "\n",
        "# Create X matrix and y vector\n",
        "y = df['y'].values\n",
        "#X = df.loc[:,['age', 'weight']].values[:, np.newaxis]\n",
        "X = df['age'].values[:, np.newaxis]\n",
        "\n",
        "# Standardize the X\n",
        "X = (X - X.mean()) /X.std()\n",
        "\n",
        "# Matrix version\n",
        "X_linalg = np.c_[np.ones(X.shape[0]), X]\n",
        "\n",
        "print(X_linalg)\n",
        "\n",
        "# Fit model\n",
        "model = LinearRegression()\n",
        "model.fit(X,y)\n",
        "\n",
        "# Assign coefficient and intercept to varibales (beta_0, beta_1)\n",
        "beta_1 = model.coef_[0]\n",
        "beta_0 = model.intercept_\n",
        "\n",
        "print('beta_1: ', beta_1)\n",
        "print('beta_0: ', beta_0)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 1.         -1.20301838]\n",
            " [ 1.         -0.39751912]\n",
            " [ 1.          0.52305147]\n",
            " [ 1.          1.21347941]\n",
            " [ 1.          0.1778375 ]\n",
            " [ 1.          1.32855074]\n",
            " [ 1.         -0.97287574]\n",
            " [ 1.         -0.16737647]\n",
            " [ 1.          0.29290882]\n",
            " [ 1.         -1.89344632]\n",
            " [ 1.          1.09840809]]\n",
            "beta_1:  12.715856751330023\n",
            "beta_0:  150.0909090909091\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JqlTbssvKFr0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "52951819-4b74-4abb-ee38-2af2653050bd"
      },
      "cell_type": "code",
      "source": [
        "y"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([132, 143, 153, 162, 154, 168, 137, 149, 159, 128, 166])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "4HwCsdC1IGe3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "outputId": "8fec2d3e-9744-4aeb-f30c-b170a5772d05"
      },
      "cell_type": "code",
      "source": [
        "# Plot sklearn version\n",
        "plt.scatter(X,y)\n",
        "# plt.xlim(0,100)\n",
        "# plt.ylim(100, 200)\n",
        "plt.xlabel('age')\n",
        "plt.ylabel('blood pressure')\n",
        "plt.title('Age vs Blood Pressure')\n",
        "plt.show();"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfkAAAFnCAYAAACl2jDXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "TaGBZbIYJNIg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Calculate using Gradient Descent"
      ]
    },
    {
      "metadata": {
        "id": "WuoSa1QxJRfe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "4a15063a-01aa-4b1a-805c-e5276b4d83e4"
      },
      "cell_type": "code",
      "source": [
        "# Choose a learning rate, alpha\n",
        "alpha = .01\n",
        "iterations = 2000\n",
        "n = len(y)\n",
        "np.random.seed(42)\n",
        "theta = [0,0]\n",
        "\n",
        "# Calculate predictions\n",
        "prediction = np.dot(X_linalg, theta)\n",
        "print('Prediction: \\n', prediction, '\\n')\n",
        "\n",
        "# Calculate error\n",
        "error = prediction - y\n",
        "print('Error: \\n', error, '\\n')\n",
        "\n",
        "# Calculate updated theta values\n",
        "theta = theta - (alpha * (1/n) * np.dot(X.T, error))\n",
        "print('Theta: \\n', theta)\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction: \n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] \n",
            "\n",
            "Error: \n",
            " [-132. -143. -153. -162. -154. -168. -137. -149. -159. -128. -166.] \n",
            "\n",
            "Theta: \n",
            " [0.12715857 0.12715857]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XjJCqwLCJRWv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ea69faa1-0cbc-43bc-aa70-cd842ab91c7c"
      },
      "cell_type": "code",
      "source": [
        "# Put it all together \n",
        "# Minimal gradient descent implementation\n",
        "\n",
        "def gradient_descent(X, y, theta, iterations, alpha):\n",
        "  for _ in range(iterations):\n",
        "    prediction = np.dot(X, theta)\n",
        "    error = prediction - y\n",
        "    updates = (alpha * (1/n) * np.dot(X.T, error))\n",
        "#     print(updates)\n",
        "    theta = theta - updates\n",
        "  return theta\n",
        "\n",
        "final_theta = gradient_descent(X_linalg, y, theta, iterations, alpha)\n",
        "print(final_theta)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[150.09090881  12.71585673]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rUsYQmHlJfP5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Multivariate"
      ]
    },
    {
      "metadata": {
        "id": "ni_EMjs5JROy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "f47c875e-f39f-4880-9d88-7e2868c060e5"
      },
      "cell_type": "code",
      "source": [
        "# Normalize dataset\n",
        "\n",
        "df_norm = (df-df.mean()) / df.std()\n",
        "\n",
        "print(df_norm.shape)\n",
        "df_norm.head()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(11, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>y</th>\n",
              "      <th>age</th>\n",
              "      <th>weight</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-1.327593</td>\n",
              "      <td>-1.147033</td>\n",
              "      <td>-1.270594</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.520363</td>\n",
              "      <td>-0.379020</td>\n",
              "      <td>-0.635297</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.213482</td>\n",
              "      <td>0.498710</td>\n",
              "      <td>-0.057754</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.873943</td>\n",
              "      <td>1.157007</td>\n",
              "      <td>0.924069</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.286867</td>\n",
              "      <td>0.169561</td>\n",
              "      <td>0.057754</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          y       age    weight\n",
              "0 -1.327593 -1.147033 -1.270594\n",
              "1 -0.520363 -0.379020 -0.635297\n",
              "2  0.213482  0.498710 -0.057754\n",
              "3  0.873943  1.157007  0.924069\n",
              "4  0.286867  0.169561  0.057754"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "metadata": {
        "id": "BlNTaqZkJRLY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "7109889b-ef0f-457f-8800-22e98141b368"
      },
      "cell_type": "code",
      "source": [
        "# Create X matrix and y vector\n",
        "y = df_norm['y'].values\n",
        "X = df_norm.iloc[:,1:3]\n",
        "\n",
        "# Matrix version\n",
        "X_linalg = np.c_[np.ones(X.shape[0]), X]\n",
        "\n",
        "#print(X_linalg)\n",
        "\n",
        "# Fit model\n",
        "model = LinearRegression()\n",
        "model.fit(X,y)\n",
        "\n",
        "\n",
        "# Assign coefficient and intercept to varibales (beta_0, beta_1)\n",
        "beta_1 = model.coef_\n",
        "beta_0 = model.intercept_\n",
        "\n",
        "print('beta_1: ', beta_1)\n",
        "print('beta_0: ', beta_0)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "beta_1:  [0.57616409 0.4254835 ]\n",
            "beta_0:  -2.949017333121613e-16\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tNnjsIBxMJD5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Cost Function"
      ]
    },
    {
      "metadata": {
        "id": "3rt_KqaUMHIP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Cost Funtion\n",
        "\n",
        "def"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Af2U_TO7NanB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e346a96c-3150-4524-e49b-181aeb90307f"
      },
      "cell_type": "code",
      "source": [
        "# Minimal gradient descent implementation\n",
        "def gradient_descent(X, y, theta, iterations, alpha):\n",
        "  for _ in range(iterations):\n",
        "    prediction = np.dot(X,theta)\n",
        "    error = prediction - y\n",
        "    updates = (alpha * (1/n) * np.dot(X.T,error))\n",
        "    theta = theta - updates\n",
        "  return theta\n",
        "\n",
        "gradient_descent(X_linalg, y, theta, iterations, alpha)\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-2.78565050e-16,  5.76164087e-01,  4.25483502e-01])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "metadata": {
        "id": "RCs6EmWhYPM-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Stretch Goals\n",
        "\n",
        "If you happen upon the most useful resources for accomplishing this challenge first, I want you to spend time today studying other variations of Gradient Descent-Based Optimizers.\n",
        "\n",
        "- Try and write a function that can perform gradient descent for arbitarily large (in dimensionality) multiple regression models. \n",
        "- Create a notebook for yourself exploring these topics\n",
        "- How do they differ from the \"vanilla\" gradient descent we explored today\n",
        "- How do these different gradient descent-based optimizers seek to overcome the challenge of finding the global minimum among various local minima?\n",
        "- Write a blog post that reteaches what you have learned about these other gradient descent-based optimizers.\n",
        "\n",
        "[Overview of GD-based optimizers](http://ruder.io/optimizing-gradient-descent/)\n",
        "\n",
        "[Siraj Raval - Evolution of Gradient Descent-Based Optimizers](https://youtu.be/nhqo0u1a6fw)"
      ]
    }
  ]
}