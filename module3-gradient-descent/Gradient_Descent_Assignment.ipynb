{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Gradient Descent Assignment.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "Qhm0Y_jqXKRv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Gradient Descent Implementation Challenge!!\n",
        "\n",
        "## Use gradient descent to find the optimal parameters of a **multiple** regression model. (We only showed an implementation for a bivariate model during lecture.)\n",
        "\n",
        "A note: Implementing gradient descent in any context is not trivial, particularly the step where we calculate the gradient will change based on the number of parameters that we're trying to optimize for. You will need to research what the gradient of a multiple regression model looks like. This challenge is pretty open-ended but I hope it will be thrilling. Please work together, help each other, share resources and generally expand your understanding of gradient descent as you try and achieve this implementation. \n",
        "\n",
        "## Suggestions:\n",
        "\n",
        "Start off with a model that has just two $X$ variables You can use any datasets that have at least two x variables. Potential candidates might be the blood pressure dataset that we used during lecture on Monday: [HERE](https://college.cengage.com/mathematics/brase/understandable_statistics/7e/students/datasets/mlr/excel/mlr02.xls) or any of the housing datasets. You would just need to select from them the two varaibles $x$ variables and one y variable that you want to work with that you most want to work with. \n",
        "\n",
        "Use Sklearn to find the optimal parameters of your model first. (like we did during the lecture.) So that you can compare the parameter estimates of your gradient-descent linear regression to the estimates of OLS linear regression. If implemented correctly they should be nearly identical.\n",
        "\n",
        "Becoming a Data Scientist is all about striking out into the unknown, getting stuck and then researching and fighting and learning until you get yourself unstuck. Work together! And fight to take your own learning-rate fueled step towards your own optimal understanding of gradient descent! \n"
      ]
    },
    {
      "metadata": {
        "id": "YEb8PI1mlCZH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "44e07069-44ee-48eb-d384-646b1a7d7201"
      },
      "cell_type": "code",
      "source": [
        "!pip3 uninstall seaborn"
      ],
      "execution_count": 252,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling seaborn-0.9.0:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.6/dist-packages/seaborn-0.9.0.dist-info/*\n",
            "    /usr/local/lib/python3.6/dist-packages/seaborn/*\n",
            "Proceed (y/n)? n\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JhkJfdddlCh2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "1a47c3a5-addd-480e-fa83-22da2d2fc5e8"
      },
      "cell_type": "code",
      "source": [
        "!pip3 --no-cache-dir install seaborn"
      ],
      "execution_count": 253,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.6/dist-packages (0.9.0)\n",
            "Requirement already satisfied: pandas>=0.15.2 in /usr/local/lib/python3.6/dist-packages (from seaborn) (0.22.0)\n",
            "Requirement already satisfied: matplotlib>=1.4.3 in /usr/local/lib/python3.6/dist-packages (from seaborn) (3.0.2)\n",
            "Requirement already satisfied: numpy>=1.9.3 in /usr/local/lib/python3.6/dist-packages (from seaborn) (1.14.6)\n",
            "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from seaborn) (1.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.15.2->seaborn) (2.5.3)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas>=0.15.2->seaborn) (2018.9)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.4.3->seaborn) (2.3.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.4.3->seaborn) (1.0.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.4.3->seaborn) (0.10.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2->pandas>=0.15.2->seaborn) (1.11.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib>=1.4.3->seaborn) (40.6.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "e0VmHG4Ky4ou",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "from scipy import stats\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler, RobustScaler, StandardScaler "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6qhtaYS3D6et",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Spring Cleaning"
      ]
    },
    {
      "metadata": {
        "id": "HpeAQTz2y-6b",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/ryanleeallred/datasets/master/Ames%20Housing%20Data/train.csv')\n",
        "df = df.drop(['Id'], axis=1)\n",
        "df = df.fillna(0)\n",
        "\n",
        "\n",
        "def clean_cat(df):\n",
        "  for col_name in df.columns:\n",
        "      if(df[col_name].dtype == 'object'):\n",
        "          df[col_name]= df[col_name].astype('category')\n",
        "          df[col_name] = df[col_name].cat.codes\n",
        "          \n",
        "\n",
        "\n",
        "clean_cat(df)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "K0gAAnPiSJH8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Kill The Outliers~"
      ]
    },
    {
      "metadata": {
        "id": "nq_hb9RZSLmu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df = df[(np.abs(stats.zscore(df)) < 3).all(axis=1)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5oxzQSn1SRmu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# There's a bump on the log in the whole at the bottom of the sea!"
      ]
    },
    {
      "metadata": {
        "id": "qyPkhV8PSWr6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df['log_price'] = np.log(df['SalePrice'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "K9yF47vkSNJq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df = df.astype('float64')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Nu1TkIcLNZhA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "69aa7070-c53f-49e5-9554-b75d125c8171"
      },
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "execution_count": 259,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(601, 81)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 259
        }
      ]
    },
    {
      "metadata": {
        "id": "M9sgfjbAh_Co",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Filter the features you want"
      ]
    },
    {
      "metadata": {
        "id": "YcRNMbcZPsXI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "df1 = df[['GrLivArea', 'OverallQual', 'TotalBsmtSF', 'GarageArea', \n",
        "               'GarageCars', 'OverallCond']]\n",
        "\n",
        "df2 = df[['SalePrice']]\n",
        "\n",
        "df_X = df1.values\n",
        "df_y = df2.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "B_d3M6myqV-l",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#'\"One road leads home and a thousand roads lead into the wilderness'"
      ]
    },
    {
      "metadata": {
        "id": "iEJvuRZoqksA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "3b85de93-0969-4fee-ced0-d3eb9c9b34ff"
      },
      "cell_type": "code",
      "source": [
        "# Slit into test and train datasets\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(df_X, df_y, test_size=.30, \n",
        "                                                    random_state=42)\n",
        "\n",
        "# i use the StadardScaler here because i took the liberty of identifying and \n",
        "# removing my outliers previously with the IQR as a baseline\n",
        "scaler = StandardScaler() \n",
        "\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "\n",
        "# do not fit the test data\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# fit model using train datasets\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, Y_train)\n",
        "\n",
        "# Create new predictions using x_test\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Measure Accuracy using y_test and y_pred\n",
        "RMSE = (np.sqrt(mean_squared_error(Y_test, y_pred)))\n",
        "R2 = r2_score(Y_test, y_pred)\n",
        "\n",
        "print('RMSE is {}'.format(RMSE))\n",
        "print('R^2 is {}'.format(R2))\n",
        "\n",
        "print(\"coefficients: \", model.coef_)\n",
        "print(\"intercepts: \", model.intercept_)\n",
        "\n",
        "\n",
        "n_weights = model.coef_[0]"
      ],
      "execution_count": 261,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RMSE is 20485.39911478344\n",
            "R^2 is 0.8757345931666435\n",
            "coefficients:  [[22923.80954878 20028.1271326  12808.56803715  6995.98580636\n",
            "   6365.11949262  3176.33604353]]\n",
            "intercepts:  [182848.59761905]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ESi6IB95rKos",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "8751bfde-a6b3-4738-f253-ac2c70906704"
      },
      "cell_type": "code",
      "source": [
        "n_weights"
      ],
      "execution_count": 262,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([22923.80954878, 20028.1271326 , 12808.56803715,  6995.98580636,\n",
              "        6365.11949262,  3176.33604353])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 262
        }
      ]
    },
    {
      "metadata": {
        "id": "SP16VFgi9L8f",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Normalin! Normalin!\n",
        "\n",
        "I want to compare gradient descent to my previous implementation of multivariate linear regression, given that the features selected were done so with a correlational matrix to log(sales price of home)."
      ]
    },
    {
      "metadata": {
        "id": "klITa2T9zL7X",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def normalize(features):\n",
        "    '''\n",
        "    features     -   (1460, 80)\n",
        "    features.T   -   (80, 1460)\n",
        "\n",
        "    We transpose the input matrix, swapping\n",
        "    cols and rows to make vector math easier\n",
        "    '''\n",
        "\n",
        "    for feature in features.T:\n",
        "        fmean = np.mean(feature)\n",
        "        frange = np.amax(feature) - np.amin(feature)\n",
        "\n",
        "        #Vector Subtraction\n",
        "        feature -= fmean\n",
        "\n",
        "        #Vector Division\n",
        "        feature /= frange\n",
        "    \n",
        "\n",
        "    return features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6yHh2JtaPFoD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_X = normalize(df_X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BBGai_qCgusO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b309293e-c128-4cff-ff28-e64db914f13a"
      },
      "cell_type": "code",
      "source": [
        "df_X.shape"
      ],
      "execution_count": 265,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(601, 6)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 265
        }
      ]
    },
    {
      "metadata": {
        "id": "W47al07S9m03",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Hypothesis\n",
        "\\begin{align}\n",
        "h\\theta(x) = \\theta_{0} + \\theta_{1}OverallQual +\\theta_{2}GrLivArea +\\theta_{3}TotalBsmtSF + \\theta_{4}GarageArea + \\theta_{5}GarageCars + \\theta_{6}OverallCond\n",
        "\\end{align}"
      ]
    },
    {
      "metadata": {
        "id": "2BCUW5Ns-7k6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Explanation of reduction to singular value by transposing theta and multiplying it by our feature matrix for multivariate linear regression\n",
        "\n",
        "Given the hypothesis, $X_{0} = 1$\n",
        "\n",
        "$X = \\begin{bmatrix}X_{0} \\\\ X_{1} \\\\ X_{2} \\\\ X_{n}\\end{bmatrix} \\varepsilon\\mathbb{R}$     $\\theta = \\begin{bmatrix}\\theta_{0} \\\\ \\theta_{1} \\\\ \\theta_{2}\\\\ \\theta_{n}\\end{bmatrix} \\varepsilon\\mathbb{R}$\n",
        "\n",
        "\n",
        "-----------------------------------------------------------------------------------------\n",
        "\n",
        "$\\theta^T = \\begin{bmatrix} \n",
        "\\theta_{0}& \\theta_{1} &...\\theta_{n}\\\\ \n",
        "\\end{bmatrix}$\n",
        "\n",
        "\n",
        "#Our $\\theta^T$ values multiplied by our X or features \n",
        "\n",
        "$\\theta_{0}X_{0} + \\theta_{1}X_{1} + \\theta_{n}X_{n} = \\theta^TX$"
      ]
    },
    {
      "metadata": {
        "id": "_zPvP7ytHOKB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Gradient Descent given Multivariate\n",
        "\n",
        "$h_\\theta(x) = \\theta^Tx$"
      ]
    },
    {
      "metadata": {
        "id": "1RQYM9AWH243",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Calculating our cost function \n",
        "\\begin{align}\n",
        "MSE =  \\frac{1}{2N} \\sum_{i=1}^{n} (y_i - (W_1 x_1 + W_2 x_2 + W_3 x_3 + W_4 x_4 + W_5 x_5 + W_6 x_6))^2\n",
        "\\end{align}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "*   Divide by 2 to make finding the derivative easier later. \n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "VX-nkHyD-5EF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# init weights\n",
        "\n",
        "w1, w2, w3, w4, w5, w6 = 22923.80954877, 20028.1271326 , 12808.56803715,  6995.98580636, 6365.11949262, 3176.33604353\n",
        "\n",
        "weights = np.array([[w1], \n",
        "                    [w2], \n",
        "                    [w3], \n",
        "                    [w4], \n",
        "                    [w5], \n",
        "                    [w6]])\n",
        "\n",
        "def cost_function(features, target, weights):\n",
        "    '''\n",
        "    returns 1d matrix of predictions \n",
        "    '''\n",
        "    \n",
        "    N = len(target)\n",
        "    \n",
        "    predictions = predict(features, weights)\n",
        "    squared_error = (predictions - target)**2\n",
        "    \n",
        "    return 1.0/(2*N) * squared_error.sum()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MpGwd7GTJhcU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c48e8144-88f6-4c24-ef79-d850d732214d"
      },
      "cell_type": "code",
      "source": [
        "weights.shape"
      ],
      "execution_count": 267,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 267
        }
      ]
    },
    {
      "metadata": {
        "id": "qTgkot8gTpLj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "weights = weights.astype('float64')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KaUB-N9xRTLS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Bad news bears...I literally spent 5 hours fighting with getting correct matrix shapes for everything for this to work. Was it worth it? we shall see....\n",
        "\n",
        "Otherwise, this is a working multivariate gradient descent. What a freaking nightmare."
      ]
    },
    {
      "metadata": {
        "id": "cY8lZ6u-RSio",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "a7ed8980-07a5-446d-80b9-aba206d2bdb8"
      },
      "cell_type": "code",
      "source": [
        "def predict(features, weights):\n",
        "  \n",
        "  return np.dot(features, weights)\n",
        "\n",
        "temp_predictions = predict(df_X, weights)\n",
        "temp_predictions[0:5]"
      ],
      "execution_count": 269,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  3608.47468672],\n",
              "       [  5355.86844436],\n",
              "       [ 18584.47874654],\n",
              "       [ 13192.37214398],\n",
              "       [-12235.3358625 ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 269
        }
      ]
    },
    {
      "metadata": {
        "id": "EdtnUikF7rV4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Finally defining gradient descent :) \n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "hbXUgy5M57vJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def gradient_descent(features, targets, weights, learning_rate):\n",
        "  \n",
        "  predictions = predict(features, weights)\n",
        "  \n",
        "  feature_length = len(features)\n",
        "  \n",
        "  #1. Time to predict!\n",
        "  predictions = predict(features, weights)\n",
        "  #2 - Calculate error/loss\n",
        "  error = targets - predictions\n",
        "  #3 Transpose features \n",
        "    # So we can multiply w the error matrix.\n",
        "    # Returns a (6,1) matrix holding 6 partial derivatives --\n",
        "    # one for each feature -- representing the aggregate\n",
        "    # slope of the cost function across all observations\n",
        "  gradient = np.dot(-features.T,  error)\n",
        "    \n",
        "  #4 Take the average error derivative for each feature\n",
        "  gradient /= feature_length\n",
        "    \n",
        "  #5 - Multiply the gradient by our learning rate\n",
        "  gradient *= learning_rate\n",
        "  \n",
        "  #6 - Subtract from our weights to minimize cost\n",
        "  weights -= gradient\n",
        "  \n",
        "  return weights\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VVBUKBS0hdoW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mesa1 = []\n",
        "mesa2 = []\n",
        "mesa3 = []\n",
        "mesa4 = []\n",
        "mesa5 = []\n",
        "mesa6 = []\n",
        "\n",
        "costs = []\n",
        "\n",
        "hue = [1, 2, 3, 4, 5, 6]\n",
        "\n",
        "for i in range(500):\n",
        "  mesa = gradient_descent(df_X, df_y, weights, .25)\n",
        "  mesa1.append(mesa[0][0])\n",
        "  mesa2.append(mesa[1][0])\n",
        "  mesa3.append(mesa[2][0])\n",
        "  mesa4.append(mesa[3][0])\n",
        "  mesa5.append(mesa[4][0])\n",
        "  mesa6.append(mesa[5][0])\n",
        "  costs.append(cost_function(df_X, df_y, weights))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lXpkcex1hoFu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "3392c9f5-faaa-409b-b13d-094b0c775539"
      },
      "cell_type": "code",
      "source": [
        "mesa"
      ],
      "execution_count": 272,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[120499.53185232],\n",
              "       [ 98940.3332763 ],\n",
              "       [ 69869.67743209],\n",
              "       [ 33318.57203556],\n",
              "       [ 32554.78021041],\n",
              "       [ 20131.3723992 ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 272
        }
      ]
    },
    {
      "metadata": {
        "id": "_UhxpFBWsziF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "costs = pd.DataFrame(costs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R4IWlkFCtxfQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "outputId": "e5f69090-97d2-419b-c2a7-bcf624d311b5"
      },
      "cell_type": "code",
      "source": [
        "costs.plot()"
      ],
      "execution_count": 274,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fc984df4240>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 274
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEDCAYAAAAyZm/jAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nWsfQK7erbXMAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "RCs6EmWhYPM-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Stretch Goals\n",
        "\n",
        "If you happen upon the most useful resources for accomplishing this challenge first, I want you to spend time today studying other variations of Gradient Descent-Based Optimizers.\n",
        "\n",
        "- Try and write a function that can perform gradient descent for arbitarily large (in dimensionality) multiple regression models. \n",
        "- Create a notebook for yourself exploring these topics\n",
        "- How do they differ from the \"vanilla\" gradient descent we explored today\n",
        "- How do these different gradient descent-based optimizers seek to overcome the challenge of finding the global minimum among various local minima?\n",
        "- Write a blog post that reteaches what you have learned about these other gradient descent-based optimizers.\n",
        "\n",
        "[Overview of GD-based optimizers](http://ruder.io/optimizing-gradient-descent/)\n",
        "\n",
        "[Siraj Raval - Evolution of Gradient Descent-Based Optimizers](https://youtu.be/nhqo0u1a6fw)"
      ]
    }
  ]
}