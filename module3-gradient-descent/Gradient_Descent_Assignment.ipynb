{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Gradient Descent Assignment.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "Qhm0Y_jqXKRv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Gradient Descent Implementation Challenge!!\n",
        "\n",
        "## Use gradient descent to find the optimal parameters of a **multiple** regression model. (We only showed an implementation for a bivariate model during lecture.)\n",
        "\n",
        "A note: Implementing gradient descent in any context is not trivial, particularly the step where we calculate the gradient will change based on the number of parameters that we're trying to optimize for. You will need to research what the gradient of a multiple regression model looks like. This challenge is pretty open-ended but I hope it will be thrilling. Please work together, help each other, share resources and generally expand your understanding of gradient descent as you try and achieve this implementation. \n",
        "\n",
        "## Suggestions:\n",
        "\n",
        "Start off with a model that has just two $X$ variables You can use any datasets that have at least two x variables. Potential candidates might be the blood pressure dataset that we used during lecture on Monday: [HERE](https://college.cengage.com/mathematics/brase/understandable_statistics/7e/students/datasets/mlr/excel/mlr02.xls) or any of the housing datasets. You would just need to select from them the two varaibles $x$ variables and one y variable that you want to work with that you most want to work with. \n",
        "\n",
        "Use Sklearn to find the optimal parameters of your model first. (like we did during the lecture.) So that you can compare the parameter estimates of your gradient-descent linear regression to the estimates of OLS linear regression. If implemented correctly they should be nearly identical.\n",
        "\n",
        "Becoming a Data Scientist is all about striking out into the unknown, getting stuck and then researching and fighting and learning until you get yourself unstuck. Work together! And fight to take your own learning-rate fueled step towards your own optimal understanding of gradient descent! \n"
      ]
    },
    {
      "metadata": {
        "id": "t4Ozdvi6ZVXe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import zipfile\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "import cufflinks\n",
        "import plotly\n",
        "import plotly.plotly as py\n",
        "import plotly.graph_objs as go\n",
        "plotly.tools.set_credentials_file(username='zangell', api_key='bs2CJxqOA2hlrJXKyeM9')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_tWzF6IqXIIq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# gradient descent class\n",
        "class GradientDescent(object):\n",
        "    \"\"\"Gradient Descent regressor.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    eta : float\n",
        "        Learning rate (between 0.0 and 1.0)\n",
        "    n_iter : int\n",
        "        Passes over the training dataset\n",
        "    precision : float\n",
        "        Stop updating weights after getting within a certain precision\n",
        "    random_state : int\n",
        "        Random number generator seed for random weight initialization\n",
        "    \n",
        "    Attributes\n",
        "    ----------\n",
        "    w_ : 1d-array\n",
        "        Weights after fitting\n",
        "    errors_ : list\n",
        "        Sum of squared errors (updates in each epoch)\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, eta=0.1, n_iter=500, precision=0.00001, random_state=1):\n",
        "        self.eta=eta\n",
        "        self.n_iter=n_iter\n",
        "        self.precision = precision\n",
        "        self.random_state=random_state\n",
        "    \n",
        "    def fit(self, X, y):\n",
        "        \"\"\" Fit training data\n",
        "        \n",
        "        Parameters\n",
        "        ---------\n",
        "        X : {array-like}, shape = [n_samples, n_features]\n",
        "            Training vectiors where n_samples is the number of\n",
        "            samples and n_features is the number of features\n",
        "        y : array-like, shape = [n_samples]\n",
        "            Target values\n",
        "            \n",
        "        Returns\n",
        "        ------\n",
        "        self : object\n",
        "        \n",
        "        \"\"\"\n",
        "        \n",
        "        rgen = np.random.RandomState(self.random_state) # initalize random number gen\n",
        "        \n",
        "        self.w_ = rgen.normal(loc=0.0, # initalize random weights close to 0\n",
        "                              scale=0.01,\n",
        "                              size = 1 + X.shape[1])\n",
        "        \n",
        "        self.errors_ = []\n",
        "        \n",
        "        for _ in range(self.n_iter): # loop through for specified iterations\n",
        "\n",
        "            # calculate and append error for this iteraiton\n",
        "            y_pred = self.predict(X)\n",
        "            sse = np.sum((y-y_pred)**2) / len(y)\n",
        "            self.errors_.append(sse)\n",
        "          \n",
        "            self.w_[1:] += self.eta * np.dot(X.T, y - y_pred) / len(y) \n",
        "            self.w_[0] += self.eta * sum(y - y_pred) / len(y) \n",
        "              \n",
        "              \n",
        "        return self\n",
        "    \n",
        "    def net_input (self, X):\n",
        "        \"\"\"Calculate net input\"\"\"\n",
        "        return np.dot(X, self.w_[1:]) + self.w_[0]\n",
        "    \n",
        "    def predict(self, X): # can be altered in other implementations\n",
        "        return self.net_input(X)\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CEuarahMbqg-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's test this class out on a Bike Sharing Dataset from UCI, found here https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset\n",
        "\n",
        "The data is in a .zip file format, so we'll need a little work here."
      ]
    },
    {
      "metadata": {
        "id": "FS-h1bTGcgI6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "5de3207a-f558-4c10-eaa5-147c691e57b6"
      },
      "cell_type": "code",
      "source": [
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00275/Bike-Sharing-Dataset.zip"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-01-17 00:42:55--  https://archive.ics.uci.edu/ml/machine-learning-databases/00275/Bike-Sharing-Dataset.zip\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.249\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.249|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 279992 (273K) [application/zip]\n",
            "Saving to: ‘Bike-Sharing-Dataset.zip.1’\n",
            "\n",
            "Bike-Sharing-Datase 100%[===================>] 273.43K   870KB/s    in 0.3s    \n",
            "\n",
            "2019-01-17 00:42:55 (870 KB/s) - ‘Bike-Sharing-Dataset.zip.1’ saved [279992/279992]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "I7ELyqWZco51",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "zip_ref = zipfile.ZipFile('Bike-Sharing-Dataset.zip', 'r')\n",
        "zip_ref.extractall()\n",
        "zip_ref.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "30pQ7rtZdDZH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "7fba9d05-6ea0-412f-8a6f-bfccf146cf9d"
      },
      "cell_type": "code",
      "source": [
        "df_bike = pd.read_csv('hour.csv')\n",
        "df_bike.head()"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>instant</th>\n",
              "      <th>dteday</th>\n",
              "      <th>season</th>\n",
              "      <th>yr</th>\n",
              "      <th>mnth</th>\n",
              "      <th>hr</th>\n",
              "      <th>holiday</th>\n",
              "      <th>weekday</th>\n",
              "      <th>workingday</th>\n",
              "      <th>weathersit</th>\n",
              "      <th>temp</th>\n",
              "      <th>atemp</th>\n",
              "      <th>hum</th>\n",
              "      <th>windspeed</th>\n",
              "      <th>casual</th>\n",
              "      <th>registered</th>\n",
              "      <th>cnt</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>2011-01-01</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.2879</td>\n",
              "      <td>0.81</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3</td>\n",
              "      <td>13</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>2011-01-01</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.22</td>\n",
              "      <td>0.2727</td>\n",
              "      <td>0.80</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8</td>\n",
              "      <td>32</td>\n",
              "      <td>40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>2011-01-01</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.22</td>\n",
              "      <td>0.2727</td>\n",
              "      <td>0.80</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5</td>\n",
              "      <td>27</td>\n",
              "      <td>32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>2011-01-01</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.2879</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3</td>\n",
              "      <td>10</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>2011-01-01</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.2879</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   instant      dteday  season  yr  mnth  hr  holiday  weekday  workingday  \\\n",
              "0        1  2011-01-01       1   0     1   0        0        6           0   \n",
              "1        2  2011-01-01       1   0     1   1        0        6           0   \n",
              "2        3  2011-01-01       1   0     1   2        0        6           0   \n",
              "3        4  2011-01-01       1   0     1   3        0        6           0   \n",
              "4        5  2011-01-01       1   0     1   4        0        6           0   \n",
              "\n",
              "   weathersit  temp   atemp   hum  windspeed  casual  registered  cnt  \n",
              "0           1  0.24  0.2879  0.81        0.0       3          13   16  \n",
              "1           1  0.22  0.2727  0.80        0.0       8          32   40  \n",
              "2           1  0.22  0.2727  0.80        0.0       5          27   32  \n",
              "3           1  0.24  0.2879  0.75        0.0       3          10   13  \n",
              "4           1  0.24  0.2879  0.75        0.0       0           1    1  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "metadata": {
        "id": "DhhkPTz9dd0a",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We will use linear regression to predict the number of bikes from the weather (temperature, humidity, and wind speed)."
      ]
    },
    {
      "metadata": {
        "id": "0X9SHaJydcRC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "features = ['atemp', 'hum', 'windspeed']\n",
        "target = 'cnt'\n",
        "\n",
        "X = df_bike[features]\n",
        "y = df_bike[target]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CeJFGpDkeK6G",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "First, let's see what coefficients linear regression comes up with."
      ]
    },
    {
      "metadata": {
        "id": "6uS5xXnwd-am",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "3a4b2499-7c89-4efb-98fe-c879d639e82c"
      },
      "cell_type": "code",
      "source": [
        "lr = LinearRegression()\n",
        "lr.fit(X, y)\n",
        "print ('Intercept:', lr.intercept_)\n",
        "print ('Coefficients', lr.coef_)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Intercept: 158.6952193250187\n",
            "Coefficients [ 409.22508774 -275.86319204   47.86020375]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "F5_Me4WEeee8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now let's see how our GradientDescent class does."
      ]
    },
    {
      "metadata": {
        "id": "9ewu9uGDed48",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "16e37de6-8abd-4467-92a8-02d31cd9385e"
      },
      "cell_type": "code",
      "source": [
        "gd = GradientDescent(eta=0.75, n_iter=2000)\n",
        "gd.fit(X,y)\n",
        "gd.w_"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 158.69521696,  409.2250888 , -275.86319034,   47.8602078 ])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "metadata": {
        "id": "6EKJlJw4l5XW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We got pretty close. I have also run this a few times with different learning rates/iterations, and it will converge.\n",
        "\n",
        "Let's see how our errors look over training epochs."
      ]
    },
    {
      "metadata": {
        "id": "mTb4L9VgmM1z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "outputId": "8ffc62ff-3b3b-4051-b956-66fee2923463"
      },
      "cell_type": "code",
      "source": [
        "epochs = np.arange(len(gd.errors_))\n",
        "\n",
        "trace = go.Scatter(\n",
        "    x = epochs,\n",
        "    y = gd.errors_\n",
        ")\n",
        "\n",
        "layout = dict(title = 'SSE Across Training Periods',\n",
        "              xaxis = dict(title = 'Training Period'),\n",
        "              yaxis = dict(title = 'Sum of Squared Errors'),\n",
        "              )\n",
        "\n",
        "data = [trace]\n",
        "\n",
        "fig = dict(data=data, layout=layout)\n",
        "\n",
        "py.iplot(fig)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~zangell/62.embed\" height=\"525px\" width=\"100%\"></iframe>"
            ],
            "text/plain": [
              "<plotly.tools.PlotlyDisplay object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "metadata": {
        "id": "RCs6EmWhYPM-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Stretch Goals\n",
        "\n",
        "If you happen upon the most useful resources for accomplishing this challenge first, I want you to spend time today studying other variations of Gradient Descent-Based Optimizers.\n",
        "\n",
        "- Try and write a function that can perform gradient descent for arbitarily large (in dimensionality) multiple regression models. \n",
        "- Create a notebook for yourself exploring these topics\n",
        "- How do they differ from the \"vanilla\" gradient descent we explored today\n",
        "- How do these different gradient descent-based optimizers seek to overcome the challenge of finding the global minimum among various local minima?\n",
        "- Write a blog post that reteaches what you have learned about these other gradient descent-based optimizers.\n",
        "\n",
        "[Overview of GD-based optimizers](http://ruder.io/optimizing-gradient-descent/)\n",
        "\n",
        "[Siraj Raval - Evolution of Gradient Descent-Based Optimizers](https://youtu.be/nhqo0u1a6fw)"
      ]
    }
  ]
}