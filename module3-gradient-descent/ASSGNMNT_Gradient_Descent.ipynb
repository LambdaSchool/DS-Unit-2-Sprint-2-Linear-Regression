{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ASSGNMNT: Gradient Descent.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/quinn-dougherty/DS-Unit-2-Sprint-2-Linear-Regression/blob/master/module3-gradient-descent/ASSGNMNT_Gradient_Descent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "Qhm0Y_jqXKRv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Gradient Descent Implementation Challenge!!\n",
        "\n",
        "## Use gradient descent to find the optimal parameters of a **multiple** regression model. (We only showed an implementation for a bivariate model during lecture.)\n",
        "\n",
        "A note: Implementing gradient descent in any context is not trivial, particularly the step where we calculate the gradient will change based on the number of parameters that we're trying to optimize for. You will need to research what the gradient of a multiple regression model looks like. This challenge is pretty open-ended but I hope it will be thrilling. Please work together, help each other, share resources and generally expand your understanding of gradient descent as you try and achieve this implementation. \n",
        "\n",
        "## Suggestions:\n",
        "\n",
        "Start off with a model that has just two $X$ variables You can use any datasets that have at least two x variables. Potential candidates might be the blood pressure dataset that we used during lecture on Monday: [HERE](https://college.cengage.com/mathematics/brase/understandable_statistics/7e/students/datasets/mlr/excel/mlr02.xls) or any of the housing datasets. You would just need to select from them the two varaibles $x$ variables and one y variable that you want to work with that you most want to work with. \n",
        "\n",
        "Use Sklearn to find the optimal parameters of your model first. (like we did during the lecture.) So that you can compare the parameter estimates of your gradient-descent linear regression to the estimates of OLS linear regression. If implemented correctly they should be nearly identical.\n",
        "\n",
        "Becoming a Data Scientist is all about striking out into the unknown, getting stuck and then researching and fighting and learning until you get yourself unstuck. Work together! And fight to take your own learning-rate fueled step towards your own optimal understanding of gradient descent! \n"
      ]
    },
    {
      "metadata": {
        "id": "_tWzF6IqXIIq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "a14df857-386a-47a3-bded-32f97f1ab263"
      },
      "cell_type": "code",
      "source": [
        "##\n",
        "### https://web.stanford.edu/~hastie/ElemStatLearn/\n",
        "\n",
        "'''Prostate data info\n",
        "\n",
        "Predictors (columns 1--8)\n",
        "\n",
        "lcavol\n",
        "lweight\n",
        "age\n",
        "lbph\n",
        "svi\n",
        "lcp\n",
        "gleason\n",
        "pgg45\n",
        "\n",
        "outcome (column 9)\n",
        "\n",
        "lpsa\n",
        "\n",
        "train/test indicator (column 10)\n",
        "\n",
        "This last column indicates which 67 observations were used as the \n",
        "\"training set\" and which 30 as the test set, as described on page 48\n",
        "in the book.\n",
        "\n",
        "There was an error in these data in the first edition of this\n",
        "book. Subject 32 had a value of 6.1 for lweight, which translates to a\n",
        "449 gm prostate! The correct value is 44.9 gm. We are grateful to\n",
        "Prof. Stephen W. Link for alerting us to this error.\n",
        "\n",
        "The features must first be scaled to have mean zero and  variance 96 (=n)\n",
        "before the analyses in Tables 3.1 and beyond.  That is, if x is the  96 by 8 matrix\n",
        "of features, we compute xp <- scale(x,TRUE,TRUE)\n",
        "\n",
        "'''\n",
        "import pandas as pd\n",
        "import altair as alt\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from functools import reduce\n",
        "\n",
        "def repeatedly(f, n): \n",
        "  def composition(f,g):\n",
        "    # \"f after g\" \n",
        "    return lambda x: f(g(x))\n",
        "  return reduce(lambda ma, mma: composition(ma, mma), [f]*n)\n",
        "\n",
        "df = pd.read_csv(\"https://web.stanford.edu/~hastie/ElemStatLearn/datasets/prostate.data\", \n",
        "                 delim_whitespace=True).select_dtypes(include=['float', 'int'])\n",
        "\n",
        "sc = StandardScaler() # Thanks for Mark for this bit\n",
        "\n",
        "df_normalized = pd.DataFrame(sc.fit_transform(df), columns=df.columns)\n",
        "\n",
        "df_normalized['ones'] = np.ones(df.shape[0])\n",
        "\n",
        "df_normalized.head()\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/data.py:625: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
            "  return self.partial_fit(X, y)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
            "  return self.fit(X, **fit_params).transform(X)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>lcavol</th>\n",
              "      <th>lweight</th>\n",
              "      <th>age</th>\n",
              "      <th>lbph</th>\n",
              "      <th>svi</th>\n",
              "      <th>lcp</th>\n",
              "      <th>gleason</th>\n",
              "      <th>pgg45</th>\n",
              "      <th>lpsa</th>\n",
              "      <th>ones</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-1.645861</td>\n",
              "      <td>-2.016634</td>\n",
              "      <td>-1.872101</td>\n",
              "      <td>-1.030029</td>\n",
              "      <td>-0.525657</td>\n",
              "      <td>-0.867655</td>\n",
              "      <td>-1.047571</td>\n",
              "      <td>-0.868957</td>\n",
              "      <td>-2.533318</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-1.999313</td>\n",
              "      <td>-0.725759</td>\n",
              "      <td>-0.791989</td>\n",
              "      <td>-1.030029</td>\n",
              "      <td>-0.525657</td>\n",
              "      <td>-0.867655</td>\n",
              "      <td>-1.047571</td>\n",
              "      <td>-0.868957</td>\n",
              "      <td>-2.299712</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-1.587021</td>\n",
              "      <td>-2.200154</td>\n",
              "      <td>1.368234</td>\n",
              "      <td>-1.030029</td>\n",
              "      <td>-0.525657</td>\n",
              "      <td>-0.867655</td>\n",
              "      <td>0.344407</td>\n",
              "      <td>-0.156155</td>\n",
              "      <td>-2.299712</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-2.178174</td>\n",
              "      <td>-0.812191</td>\n",
              "      <td>-0.791989</td>\n",
              "      <td>-1.030029</td>\n",
              "      <td>-0.525657</td>\n",
              "      <td>-0.867655</td>\n",
              "      <td>-1.047571</td>\n",
              "      <td>-0.868957</td>\n",
              "      <td>-2.299712</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.510513</td>\n",
              "      <td>-0.461218</td>\n",
              "      <td>-0.251933</td>\n",
              "      <td>-1.030029</td>\n",
              "      <td>-0.525657</td>\n",
              "      <td>-0.867655</td>\n",
              "      <td>-1.047571</td>\n",
              "      <td>-0.868957</td>\n",
              "      <td>-1.834631</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     lcavol   lweight       age      lbph       svi       lcp   gleason  \\\n",
              "0 -1.645861 -2.016634 -1.872101 -1.030029 -0.525657 -0.867655 -1.047571   \n",
              "1 -1.999313 -0.725759 -0.791989 -1.030029 -0.525657 -0.867655 -1.047571   \n",
              "2 -1.587021 -2.200154  1.368234 -1.030029 -0.525657 -0.867655  0.344407   \n",
              "3 -2.178174 -0.812191 -0.791989 -1.030029 -0.525657 -0.867655 -1.047571   \n",
              "4 -0.510513 -0.461218 -0.251933 -1.030029 -0.525657 -0.867655 -1.047571   \n",
              "\n",
              "      pgg45      lpsa  ones  \n",
              "0 -0.868957 -2.533318   1.0  \n",
              "1 -0.868957 -2.299712   1.0  \n",
              "2 -0.156155 -2.299712   1.0  \n",
              "3 -0.868957 -2.299712   1.0  \n",
              "4 -0.868957 -1.834631   1.0  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "metadata": {
        "id": "WhUjX-8MP-Um",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def LR(X, y, testsize=0.3): \n",
        "  #print(X.shape, y.shape)\n",
        "  # X is a dataframe with arbitrary features\n",
        "  # y is a dataframe with one feature\n",
        "  # they are each sliced from the same master df. \n",
        "  \n",
        "  # Split into test and train datasets\n",
        "  X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=testsize, random_state=42)\n",
        "\n",
        "  # fit model using train datasets\n",
        "  model = LinearRegression()\n",
        "  model.fit(X_train, Y_train)\n",
        "\n",
        "  # Create new predictions using x_test\n",
        "  y_pred = model.predict(X_test)\n",
        "\n",
        "  # Measure Accuracy using y_test and y_pred\n",
        "  RMSE = (np.sqrt(mean_squared_error(Y_test, y_pred)))\n",
        "  R2 = r2_score(Y_test, y_pred)\n",
        "\n",
        "  print('RMSE is {}'.format(RMSE))\n",
        "  print('R^2 is {}'.format(R2))\n",
        "\n",
        "  print(\"coefficients: \", model.coef_)\n",
        "  print(\"intercepts: \", model.intercept_)\n",
        "  \n",
        "  return {'RMSE': RMSE, 'R2': R2, 'coefficients': model.coef_, 'intercept': model.intercept_}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DA8LU8fTQKRd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "946541b3-8c4d-467e-a7c9-3ee07e29c19e"
      },
      "cell_type": "code",
      "source": [
        "X = df_normalized[['ones', 'lweight']]\n",
        "y = df_normalized.lpsa\n",
        "\n",
        "from_sklearn = LR(X.drop('ones', axis=1),y)\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RMSE is 0.8453364429009119\n",
            "R^2 is 0.19086153418651863\n",
            "coefficients:  [0.4165919]\n",
            "intercepts:  -0.007534350939092119\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tMeV2J9TQiG8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class GDOLSR: \n",
        "  def __init__(self, df, feat, target, epochs=1000, gamma=0.017):\n",
        "    self.df = df\n",
        "    self.X = df[['ones', feat]]\n",
        "    self.y = df[target]\n",
        "    self.theta = np.ones(X.shape[1])[:, np.newaxis]\n",
        "    self.epochs = epochs # this code really doesn't use this value for anything at all\n",
        "    self.gamma = gamma\n",
        "    self.N = self.X.shape[1]\n",
        "    self.feat = feat\n",
        "\n",
        "  def yhat(self, theta): \n",
        "    # long form: X.ones1 * theta[0] + X.lcavol * theta[1]\n",
        "    return pd.DataFrame(np.matmul(self.X, theta))\n",
        "\n",
        "  def cost(self, theta): \n",
        "    sqrd_residuals = (self.y[:, np.newaxis] - self.yhat(theta)) ** 2\n",
        "    return ((1/self.N) * sqrd_residuals.sum().values)[0]\n",
        "\n",
        "  def del_cost_0(self, theta): \n",
        "    return np.divide(-2,self.N) * (self.y[:, np.newaxis] - self.yhat(theta)).sum()\n",
        "\n",
        "  def del_cost_1(self, theta): \n",
        "    return np.divide(-2,self.N) * (self.X[self.feat][:, np.newaxis] * (self.y[:, np.newaxis] - self.yhat(theta))).sum()\n",
        "\n",
        "  # algorithm: repeat theta_k <- theta_k - alpha * del H / del theta_k `epochs` times, for k in range(len(theta))\n",
        "\n",
        "  def update(self, theta): \n",
        "    return [theta[0] - self.gamma * self.del_cost_0(theta).values, theta[1] - self.gamma * self.del_cost_1(theta).values]\n",
        "\n",
        "  def update_25x(self, the):\n",
        "    return repeatedly(self.update, 25)(the)\n",
        "  \n",
        "  def update_100x(self, the): \n",
        "    return repeatedly(self.update_25x, 4)(the)\n",
        "\n",
        "\n",
        "test = GDOLSR(df_normalized, 'lweight', 'lpsa', gamma=0.0015)\n",
        "\n",
        "theta_from_sklearn = [from_sklearn['intercept'], from_sklearn['coefficients'][0]]\n",
        "\n",
        "\n",
        "np.testing.assert_almost_equal(test.cost(theta_from_sklearn), test.cost(test.update_25x(test.theta)), 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4CedaNT9LVyE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "make_dict = {f'{x:.5}': (lambda mod: mod.cost(mod.update_100x(mod.theta)))(GDOLSR(df_normalized, 'lweight','lpsa', gamma=x)) for x in np.linspace(2**(-32),.02,100)[:-1]}\n",
        "\n",
        "# run once because it's very expensive. "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4Ib7F2cUMMSv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "c940ae8d-4512-48e4-ba89-a80893d6224b"
      },
      "cell_type": "code",
      "source": [
        "benchmark = pd.DataFrame.from_dict(make_dict, orient='index').reset_index().rename(columns={'index': 'gamma', 0: 'cost'})\n",
        "\n",
        "\n",
        "C = alt.Chart(benchmark, width=550).mark_line().encode(\n",
        "    x = alt.X('gamma', axis=alt.Axis(tickCount=10)), \n",
        "    y = alt.Y('cost', scale=alt.Scale(type='log', base=2**10))\n",
        ").interactive(bind_x=True)# , bind_y=True)\n",
        "\n",
        "C"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Chart({\n",
              "  data:          gamma        cost\n",
              "  0   2.3283e-10  103.467730\n",
              "  1   0.00020202   40.617143\n",
              "  2   0.00040404   39.414942\n",
              "  3   0.00060606   39.393714\n",
              "  4   0.00080808   39.393369\n",
              "  5    0.0010101   39.393364\n",
              "  6    0.0012121   39.393364\n",
              "  7    0.0014141   39.393364\n",
              "  8    0.0016162   39.393364\n",
              "  9    0.0018182   39.393364\n",
              "  10   0.0020202   39.393364\n",
              "  11   0.0022222   39.393364\n",
              "  12   0.0024242   39.393364\n",
              "  13   0.0026263   39.393364\n",
              "  14   0.0028283   39.393364\n",
              "  15   0.0030303   39.393364\n",
              "  16   0.0032323   39.393364\n",
              "  17   0.0034343   39.393364\n",
              "  18   0.0036364   39.393364\n",
              "  19   0.0038384   39.393364\n",
              "  20   0.0040404   39.393364\n",
              "  21   0.0042424   39.393364\n",
              "  22   0.0044444   39.393364\n",
              "  23   0.0046465   39.393364\n",
              "  24   0.0048485   39.393364\n",
              "  25   0.0050505   39.393364\n",
              "  26   0.0052525   39.393364\n",
              "  27   0.0054545   39.393364\n",
              "  28   0.0056566   39.393364\n",
              "  29   0.0058586   39.393364\n",
              "  ..         ...         ...\n",
              "  69    0.013939   39.393364\n",
              "  70    0.014141   39.393364\n",
              "  71    0.014343   39.393364\n",
              "  72    0.014545   39.393364\n",
              "  73    0.014747   39.393364\n",
              "  74    0.014949   39.393364\n",
              "  75    0.015152   39.393364\n",
              "  76    0.015354   39.393364\n",
              "  77    0.015556   39.393364\n",
              "  78    0.015758   39.393364\n",
              "  79     0.01596   39.393364\n",
              "  80    0.016162   39.393364\n",
              "  81    0.016364   39.393364\n",
              "  82    0.016566   39.393364\n",
              "  83    0.016768   39.393364\n",
              "  84     0.01697   39.393364\n",
              "  85    0.017172   39.393364\n",
              "  86    0.017374   39.393364\n",
              "  87    0.017576   39.393364\n",
              "  88    0.017778   39.393364\n",
              "  89     0.01798   39.393364\n",
              "  90    0.018182   39.393364\n",
              "  91    0.018384   39.393364\n",
              "  92    0.018586   39.393364\n",
              "  93    0.018788   39.393364\n",
              "  94     0.01899   39.393364\n",
              "  95    0.019192   39.393364\n",
              "  96    0.019394   39.393364\n",
              "  97    0.019596   39.393364\n",
              "  98    0.019798   39.393368\n",
              "  \n",
              "  [99 rows x 2 columns],\n",
              "  encoding: EncodingWithFacet({\n",
              "    x: X({\n",
              "      axis: Axis({\n",
              "        tickCount: 10\n",
              "      }),\n",
              "      shorthand: 'gamma'\n",
              "    }),\n",
              "    y: Y({\n",
              "      scale: Scale({\n",
              "        base: 1024,\n",
              "        type: 'log'\n",
              "      }),\n",
              "      shorthand: 'cost'\n",
              "    })\n",
              "  }),\n",
              "  mark: 'line',\n",
              "  selection: SelectionMapping({\n",
              "    selector015: SelectionDef({\n",
              "      bind: 'scales',\n",
              "      encodings: ['x', 'y'],\n",
              "      type: 'interval'\n",
              "    })\n",
              "  }),\n",
              "  width: 550\n",
              "})"
            ],
            "text/html": [
              "<!DOCTYPE html>\n",
              "<html>\n",
              "<head>\n",
              "  <style>\n",
              "    .vega-actions a {\n",
              "        margin-right: 12px;\n",
              "        color: #757575;\n",
              "        font-weight: normal;\n",
              "        font-size: 13px;\n",
              "    }\n",
              "    .error {\n",
              "        color: red;\n",
              "    }\n",
              "  </style>\n",
              "  <script type=\"text/javascript\" src=\"https://cdn.jsdelivr.net/npm//vega@4\"></script>\n",
              "  <script type=\"text/javascript\" src=\"https://cdn.jsdelivr.net/npm//vega-lite@2.6.0\"></script>\n",
              "  <script type=\"text/javascript\" src=\"https://cdn.jsdelivr.net/npm//vega-embed@3\"></script>\n",
              "</head>\n",
              "<body>\n",
              "  <div id=\"altair-viz\"></div>\n",
              "  <script>\n",
              "      var spec = {\"config\": {\"view\": {\"width\": 400, \"height\": 300}}, \"data\": {\"name\": \"data-998daaa31831a87cce068142b8604b5b\"}, \"mark\": \"line\", \"encoding\": {\"x\": {\"type\": \"nominal\", \"axis\": {\"tickCount\": 10}, \"field\": \"gamma\"}, \"y\": {\"type\": \"quantitative\", \"field\": \"cost\", \"scale\": {\"base\": 1024, \"type\": \"log\"}}}, \"selection\": {\"selector015\": {\"type\": \"interval\", \"bind\": \"scales\", \"encodings\": [\"x\", \"y\"]}}, \"width\": 550, \"$schema\": \"https://vega.github.io/schema/vega-lite/v2.6.0.json\", \"datasets\": {\"data-998daaa31831a87cce068142b8604b5b\": [{\"gamma\": \"2.3283e-10\", \"cost\": 103.46773047912549}, {\"gamma\": \"0.00020202\", \"cost\": 40.61714255055814}, {\"gamma\": \"0.00040404\", \"cost\": 39.41494229263282}, {\"gamma\": \"0.00060606\", \"cost\": 39.393714264204434}, {\"gamma\": \"0.00080808\", \"cost\": 39.39336937703215}, {\"gamma\": \"0.0010101\", \"cost\": 39.39336423946072}, {\"gamma\": \"0.0012121\", \"cost\": 39.39336416955173}, {\"gamma\": \"0.0014141\", \"cost\": 39.393364168686205}, {\"gamma\": \"0.0016162\", \"cost\": 39.393364168676506}, {\"gamma\": \"0.0018182\", \"cost\": 39.3933641686764}, {\"gamma\": \"0.0020202\", \"cost\": 39.3933641686764}, {\"gamma\": \"0.0022222\", \"cost\": 39.393364168676406}, {\"gamma\": \"0.0024242\", \"cost\": 39.3933641686764}, {\"gamma\": \"0.0026263\", \"cost\": 39.3933641686764}, {\"gamma\": \"0.0028283\", \"cost\": 39.393364168676406}, {\"gamma\": \"0.0030303\", \"cost\": 39.3933641686764}, {\"gamma\": \"0.0032323\", \"cost\": 39.3933641686764}, {\"gamma\": \"0.0034343\", \"cost\": 39.393364168676406}, {\"gamma\": \"0.0036364\", \"cost\": 39.393364168676406}, {\"gamma\": \"0.0038384\", \"cost\": 39.393364168676406}, {\"gamma\": \"0.0040404\", \"cost\": 39.393364168676406}, {\"gamma\": \"0.0042424\", \"cost\": 39.393364168676406}, {\"gamma\": \"0.0044444\", \"cost\": 39.393364168676406}, {\"gamma\": \"0.0046465\", \"cost\": 39.393364168676406}, {\"gamma\": \"0.0048485\", \"cost\": 39.393364168676406}, {\"gamma\": \"0.0050505\", \"cost\": 39.393364168676406}, {\"gamma\": \"0.0052525\", \"cost\": 39.393364168676406}, {\"gamma\": \"0.0054545\", \"cost\": 39.393364168676406}, {\"gamma\": \"0.0056566\", \"cost\": 39.393364168676406}, {\"gamma\": \"0.0058586\", \"cost\": 39.393364168676406}, {\"gamma\": \"0.0060606\", \"cost\": 39.393364168676406}, {\"gamma\": \"0.0062626\", \"cost\": 39.393364168676406}, {\"gamma\": \"0.0064646\", \"cost\": 39.393364168676406}, {\"gamma\": \"0.0066667\", \"cost\": 39.393364168676406}, {\"gamma\": \"0.0068687\", \"cost\": 39.393364168676406}, {\"gamma\": \"0.0070707\", \"cost\": 39.393364168676406}, {\"gamma\": \"0.0072727\", \"cost\": 39.393364168676406}, {\"gamma\": \"0.0074747\", \"cost\": 39.393364168676406}, {\"gamma\": \"0.0076768\", \"cost\": 39.393364168676406}, {\"gamma\": \"0.0078788\", \"cost\": 39.393364168676406}, {\"gamma\": \"0.0080808\", \"cost\": 39.393364168676406}, {\"gamma\": \"0.0082828\", \"cost\": 39.393364168676406}, {\"gamma\": \"0.0084848\", \"cost\": 39.393364168676406}, {\"gamma\": \"0.0086869\", \"cost\": 39.393364168676406}, {\"gamma\": \"0.0088889\", \"cost\": 39.393364168676406}, {\"gamma\": \"0.0090909\", \"cost\": 39.393364168676406}, {\"gamma\": \"0.0092929\", \"cost\": 39.393364168676406}, {\"gamma\": \"0.0094949\", \"cost\": 39.393364168676406}, {\"gamma\": \"0.009697\", \"cost\": 39.393364168676406}, {\"gamma\": \"0.009899\", \"cost\": 39.393364168676406}, {\"gamma\": \"0.010101\", \"cost\": 39.393364168676406}, {\"gamma\": \"0.010303\", \"cost\": 39.393364168676406}, {\"gamma\": \"0.010505\", \"cost\": 39.393364168676406}, {\"gamma\": \"0.010707\", \"cost\": 39.393364168676406}, {\"gamma\": \"0.010909\", \"cost\": 39.393364168676406}, {\"gamma\": \"0.011111\", \"cost\": 39.393364168676406}, {\"gamma\": \"0.011313\", \"cost\": 39.393364168676406}, {\"gamma\": \"0.011515\", \"cost\": 39.393364168676406}, {\"gamma\": \"0.011717\", \"cost\": 39.393364168676406}, {\"gamma\": \"0.011919\", \"cost\": 39.393364168676406}, {\"gamma\": \"0.012121\", \"cost\": 39.393364168676406}, {\"gamma\": \"0.012323\", \"cost\": 39.393364168676406}, {\"gamma\": \"0.012525\", \"cost\": 39.393364168676406}, {\"gamma\": \"0.012727\", \"cost\": 39.393364168676406}, {\"gamma\": \"0.012929\", \"cost\": 39.393364168676406}, {\"gamma\": \"0.013131\", \"cost\": 39.393364168676406}, {\"gamma\": \"0.013333\", \"cost\": 39.393364168676406}, {\"gamma\": \"0.013535\", \"cost\": 39.393364168676406}, {\"gamma\": \"0.013737\", \"cost\": 39.393364168676406}, {\"gamma\": \"0.013939\", \"cost\": 39.393364168676406}, {\"gamma\": \"0.014141\", \"cost\": 39.393364168676406}, {\"gamma\": \"0.014343\", \"cost\": 39.393364168676406}, {\"gamma\": \"0.014545\", \"cost\": 39.393364168676406}, {\"gamma\": \"0.014747\", \"cost\": 39.393364168676406}, {\"gamma\": \"0.014949\", \"cost\": 39.393364168676406}, {\"gamma\": \"0.015152\", \"cost\": 39.393364168676406}, {\"gamma\": \"0.015354\", \"cost\": 39.393364168676406}, {\"gamma\": \"0.015556\", \"cost\": 39.393364168676406}, {\"gamma\": \"0.015758\", \"cost\": 39.393364168676406}, {\"gamma\": \"0.01596\", \"cost\": 39.393364168676406}, {\"gamma\": \"0.016162\", \"cost\": 39.393364168676406}, {\"gamma\": \"0.016364\", \"cost\": 39.393364168676406}, {\"gamma\": \"0.016566\", \"cost\": 39.393364168676406}, {\"gamma\": \"0.016768\", \"cost\": 39.393364168676406}, {\"gamma\": \"0.01697\", \"cost\": 39.393364168676406}, {\"gamma\": \"0.017172\", \"cost\": 39.393364168676406}, {\"gamma\": \"0.017374\", \"cost\": 39.393364168676406}, {\"gamma\": \"0.017576\", \"cost\": 39.3933641686764}, {\"gamma\": \"0.017778\", \"cost\": 39.393364168676406}, {\"gamma\": \"0.01798\", \"cost\": 39.393364168676406}, {\"gamma\": \"0.018182\", \"cost\": 39.393364168676406}, {\"gamma\": \"0.018384\", \"cost\": 39.3933641686764}, {\"gamma\": \"0.018586\", \"cost\": 39.393364168676406}, {\"gamma\": \"0.018788\", \"cost\": 39.393364168676406}, {\"gamma\": \"0.01899\", \"cost\": 39.39336416867647}, {\"gamma\": \"0.019192\", \"cost\": 39.393364168683796}, {\"gamma\": \"0.019394\", \"cost\": 39.39336416934134}, {\"gamma\": \"0.019596\", \"cost\": 39.39336422276939}, {\"gamma\": \"0.019798\", \"cost\": 39.393368171713334}]}};\n",
              "      var embedOpt = {\"mode\": \"vega-lite\"};\n",
              "\n",
              "      function showError(el, error){\n",
              "          el.innerHTML = ('<div class=\"error\" style=\"color:red;\">'\n",
              "                          + '<p>JavaScript Error: ' + error.message + '</p>'\n",
              "                          + \"<p>This usually means there's a typo in your chart specification. \"\n",
              "                          + \"See the javascript console for the full traceback.</p>\"\n",
              "                          + '</div>');\n",
              "          throw error;\n",
              "      }\n",
              "      const el = document.getElementById('altair-viz');\n",
              "      vegaEmbed(\"#altair-viz\", spec, embedOpt)\n",
              "        .catch(error => showError(el, error));\n",
              "\n",
              "  </script>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "metadata": {
        "id": "2teqUvYbMMNn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "outputId": "a9b6a184-110c-46bf-f38e-5c3395493929"
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "benchmark['ln_cost'] = np.log(benchmark.cost)\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "ax.plot(benchmark['gamma'], benchmark['ln_cost'], linewidth=50, visible=True, alpha=1)\n",
        "plt.xlabel('gamma')\n",
        "plt.ylabel('cost')\n",
        "\n",
        "plt.xlim([0.02, 0.00001])\n",
        "plt.ylim([20, 1000])\n",
        "plt.yscale('log')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAg0AAAFeCAYAAAAG6ixQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUPI/slc2pu2MUWgAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f2235db5780>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "RABrnEP4LVuv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Giyy9d3MLVoK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "c5yDF040QiFm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wRQI_RrwQiEL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "outputId": "73a67eb9-bce1-4004-e281-aa31fe2d7482"
      },
      "cell_type": "code",
      "source": [
        "np.linspace(0,0.1,100)[:-1]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.        , 0.0010101 , 0.0020202 , 0.0030303 , 0.0040404 ,\n",
              "       0.00505051, 0.00606061, 0.00707071, 0.00808081, 0.00909091,\n",
              "       0.01010101, 0.01111111, 0.01212121, 0.01313131, 0.01414141,\n",
              "       0.01515152, 0.01616162, 0.01717172, 0.01818182, 0.01919192,\n",
              "       0.02020202, 0.02121212, 0.02222222, 0.02323232, 0.02424242,\n",
              "       0.02525253, 0.02626263, 0.02727273, 0.02828283, 0.02929293,\n",
              "       0.03030303, 0.03131313, 0.03232323, 0.03333333, 0.03434343,\n",
              "       0.03535354, 0.03636364, 0.03737374, 0.03838384, 0.03939394,\n",
              "       0.04040404, 0.04141414, 0.04242424, 0.04343434, 0.04444444,\n",
              "       0.04545455, 0.04646465, 0.04747475, 0.04848485, 0.04949495,\n",
              "       0.05050505, 0.05151515, 0.05252525, 0.05353535, 0.05454545,\n",
              "       0.05555556, 0.05656566, 0.05757576, 0.05858586, 0.05959596,\n",
              "       0.06060606, 0.06161616, 0.06262626, 0.06363636, 0.06464646,\n",
              "       0.06565657, 0.06666667, 0.06767677, 0.06868687, 0.06969697,\n",
              "       0.07070707, 0.07171717, 0.07272727, 0.07373737, 0.07474747,\n",
              "       0.07575758, 0.07676768, 0.07777778, 0.07878788, 0.07979798,\n",
              "       0.08080808, 0.08181818, 0.08282828, 0.08383838, 0.08484848,\n",
              "       0.08585859, 0.08686869, 0.08787879, 0.08888889, 0.08989899,\n",
              "       0.09090909, 0.09191919, 0.09292929, 0.09393939, 0.09494949,\n",
              "       0.0959596 , 0.0969697 , 0.0979798 , 0.0989899 ])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "3GdnhUE1QiCF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fSx7cCA7Qh_a",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-7uGJZLv2T-V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5031ac02-f106-427f-abda-79a10c46f176"
      },
      "cell_type": "code",
      "source": [
        "2**10"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1024"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "RCs6EmWhYPM-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Stretch Goals\n",
        "\n",
        "If you happen upon the most useful resources for accomplishing this challenge first, I want you to spend time today studying other variations of Gradient Descent-Based Optimizers.\n",
        "\n",
        "- Try and write a function that can perform gradient descent for arbitarily large (in dimensionality) multiple regression models. \n",
        "- Create a notebook for yourself exploring these topics\n",
        "- How do they differ from the \"vanilla\" gradient descent we explored today\n",
        "- How do these different gradient descent-based optimizers seek to overcome the challenge of finding the global minimum among various local minima?\n",
        "- Write a blog post that reteaches what you have learned about these other gradient descent-based optimizers.\n",
        "\n",
        "[Overview of GD-based optimizers](http://ruder.io/optimizing-gradient-descent/)\n",
        "\n",
        "[Siraj Raval - Evolution of Gradient Descent-Based Optimizers](https://youtu.be/nhqo0u1a6fw)"
      ]
    },
    {
      "metadata": {
        "id": "sgmw05C6dZmB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# What you need to know about derivatives for this\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "c8zMsHuVCsov",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lg7X9tZJCslp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "466fecae-fd3a-4dfb-fe25-396b82ceadc2"
      },
      "cell_type": "code",
      "source": [
        "class A: \n",
        "  def __init__(self, p, q, r=5): \n",
        "    self.p = p\n",
        "    self.q = q\n",
        "    self.r = r\n",
        "    self.s = p ** q / r\n",
        "  \n",
        "  def getvals(self): \n",
        "    print(self.p, self.q, self.r, self.s)\n",
        "    return (self.p,self.q,self.r,self.s)\n",
        "\n",
        "aaaaaa = A(1,2)\n",
        "\n",
        "aaaaaa.getvals()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 2 5 0.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 2, 5, 0.2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    }
  ]
}