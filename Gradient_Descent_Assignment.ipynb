{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Gradient Descent Assignment.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tesseract314/DS-Unit-2-Sprint-2-Linear-Regression/blob/master/Gradient_Descent_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "Qhm0Y_jqXKRv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Gradient Descent Implementation Challenge!!\n",
        "\n",
        "## Use gradient descent to find the optimal parameters of a **multiple** regression model. (We only showed an implementation for a bivariate model during lecture.)\n",
        "\n",
        "A note: Implementing gradient descent in any context is not trivial, particularly the step where we calculate the gradient will change based on the number of parameters that we're trying to optimize for. You will need to research what the gradient of a multiple regression model looks like. This challenge is pretty open-ended but I hope it will be thrilling. Please work together, help each other, share resources and generally expand your understanding of gradient descent as you try and achieve this implementation. \n",
        "\n",
        "## Suggestions:\n",
        "\n",
        "Start off with a model that has just two $X$ variables You can use any datasets that have at least two $x$ variables. Potential candidates might be the blood pressure dataset that we used during lecture on Monday: [HERE](https://college.cengage.com/mathematics/brase/understandable_statistics/7e/students/datasets/mlr/excel/mlr02.xls) or any of the housing datasets. You would just need to select from them the two $x$ variables and one $y$ variable that you want to work with. \n",
        "\n",
        "Use Sklearn to find the optimal parameters of your model first. (like we did during the lecture.) So that you can compare the parameter estimates of your gradient-descent linear regression to the estimates of OLS linear regression. If implemented correctly they should be nearly identical.\n",
        "\n",
        "Becoming a Data Scientist is all about striking out into the unknown, getting stuck and then researching and fighting and learning until you get yourself unstuck. Work together! And fight to take your own learning-rate fueled step towards your own optimal understanding of gradient descent! \n"
      ]
    },
    {
      "metadata": {
        "id": "_Xzf_Wfvgsek",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from pandas.plotting import scatter_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7oH0JU4HhLMd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "59ea92b5-068d-4bee-9539-b50c79590321"
      },
      "cell_type": "code",
      "source": [
        "# Importing dataset and reading data\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/ryanleeallred/datasets/master/car_regression.csv')\n",
        "df.head()"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>make</th>\n",
              "      <th>price</th>\n",
              "      <th>body</th>\n",
              "      <th>mileage</th>\n",
              "      <th>engV</th>\n",
              "      <th>engType</th>\n",
              "      <th>registration</th>\n",
              "      <th>year</th>\n",
              "      <th>drive</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>23</td>\n",
              "      <td>15500.0</td>\n",
              "      <td>0</td>\n",
              "      <td>68</td>\n",
              "      <td>2.5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2010</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>50</td>\n",
              "      <td>20500.0</td>\n",
              "      <td>3</td>\n",
              "      <td>173</td>\n",
              "      <td>1.8</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2011</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>50</td>\n",
              "      <td>35000.0</td>\n",
              "      <td>2</td>\n",
              "      <td>135</td>\n",
              "      <td>5.5</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>2008</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>50</td>\n",
              "      <td>17800.0</td>\n",
              "      <td>5</td>\n",
              "      <td>162</td>\n",
              "      <td>1.8</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2012</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>55</td>\n",
              "      <td>16600.0</td>\n",
              "      <td>0</td>\n",
              "      <td>83</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>2013</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   make    price  body  mileage  engV  engType  registration  year  drive\n",
              "0    23  15500.0     0       68   2.5        1             1  2010      1\n",
              "1    50  20500.0     3      173   1.8        1             1  2011      2\n",
              "2    50  35000.0     2      135   5.5        3             1  2008      2\n",
              "3    50  17800.0     5      162   1.8        0             1  2012      0\n",
              "4    55  16600.0     0       83   2.0        3             1  2013      1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "metadata": {
        "id": "UsJT-uY-kqU6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        },
        "outputId": "2422c865-d682-4103-887c-a91f36226082"
      },
      "cell_type": "code",
      "source": [
        "df.corr()"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>make</th>\n",
              "      <th>price</th>\n",
              "      <th>body</th>\n",
              "      <th>mileage</th>\n",
              "      <th>engV</th>\n",
              "      <th>engType</th>\n",
              "      <th>registration</th>\n",
              "      <th>year</th>\n",
              "      <th>drive</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>make</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.083205</td>\n",
              "      <td>0.026127</td>\n",
              "      <td>-0.035097</td>\n",
              "      <td>-0.021246</td>\n",
              "      <td>-0.011880</td>\n",
              "      <td>0.104557</td>\n",
              "      <td>-0.013833</td>\n",
              "      <td>-0.196444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>price</th>\n",
              "      <td>-0.083205</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.253726</td>\n",
              "      <td>-0.336940</td>\n",
              "      <td>0.066226</td>\n",
              "      <td>-0.021051</td>\n",
              "      <td>0.124062</td>\n",
              "      <td>0.393562</td>\n",
              "      <td>0.221952</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>body</th>\n",
              "      <td>0.026127</td>\n",
              "      <td>-0.253726</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.287060</td>\n",
              "      <td>-0.025320</td>\n",
              "      <td>-0.184007</td>\n",
              "      <td>-0.082643</td>\n",
              "      <td>-0.154554</td>\n",
              "      <td>-0.162235</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mileage</th>\n",
              "      <td>-0.035097</td>\n",
              "      <td>-0.336940</td>\n",
              "      <td>0.287060</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.042081</td>\n",
              "      <td>-0.229644</td>\n",
              "      <td>-0.229354</td>\n",
              "      <td>-0.503135</td>\n",
              "      <td>0.067962</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>engV</th>\n",
              "      <td>-0.021246</td>\n",
              "      <td>0.066226</td>\n",
              "      <td>-0.025320</td>\n",
              "      <td>0.042081</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.004896</td>\n",
              "      <td>-0.015274</td>\n",
              "      <td>-0.041076</td>\n",
              "      <td>0.084756</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>engType</th>\n",
              "      <td>-0.011880</td>\n",
              "      <td>-0.021051</td>\n",
              "      <td>-0.184007</td>\n",
              "      <td>-0.229644</td>\n",
              "      <td>-0.004896</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.152988</td>\n",
              "      <td>-0.040471</td>\n",
              "      <td>-0.044859</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>registration</th>\n",
              "      <td>0.104557</td>\n",
              "      <td>0.124062</td>\n",
              "      <td>-0.082643</td>\n",
              "      <td>-0.229354</td>\n",
              "      <td>-0.015274</td>\n",
              "      <td>0.152988</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.185104</td>\n",
              "      <td>-0.055793</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>year</th>\n",
              "      <td>-0.013833</td>\n",
              "      <td>0.393562</td>\n",
              "      <td>-0.154554</td>\n",
              "      <td>-0.503135</td>\n",
              "      <td>-0.041076</td>\n",
              "      <td>-0.040471</td>\n",
              "      <td>0.185104</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.169709</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>drive</th>\n",
              "      <td>-0.196444</td>\n",
              "      <td>0.221952</td>\n",
              "      <td>-0.162235</td>\n",
              "      <td>0.067962</td>\n",
              "      <td>0.084756</td>\n",
              "      <td>-0.044859</td>\n",
              "      <td>-0.055793</td>\n",
              "      <td>-0.169709</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  make     price      body   mileage      engV   engType  \\\n",
              "make          1.000000 -0.083205  0.026127 -0.035097 -0.021246 -0.011880   \n",
              "price        -0.083205  1.000000 -0.253726 -0.336940  0.066226 -0.021051   \n",
              "body          0.026127 -0.253726  1.000000  0.287060 -0.025320 -0.184007   \n",
              "mileage      -0.035097 -0.336940  0.287060  1.000000  0.042081 -0.229644   \n",
              "engV         -0.021246  0.066226 -0.025320  0.042081  1.000000 -0.004896   \n",
              "engType      -0.011880 -0.021051 -0.184007 -0.229644 -0.004896  1.000000   \n",
              "registration  0.104557  0.124062 -0.082643 -0.229354 -0.015274  0.152988   \n",
              "year         -0.013833  0.393562 -0.154554 -0.503135 -0.041076 -0.040471   \n",
              "drive        -0.196444  0.221952 -0.162235  0.067962  0.084756 -0.044859   \n",
              "\n",
              "              registration      year     drive  \n",
              "make              0.104557 -0.013833 -0.196444  \n",
              "price             0.124062  0.393562  0.221952  \n",
              "body             -0.082643 -0.154554 -0.162235  \n",
              "mileage          -0.229354 -0.503135  0.067962  \n",
              "engV             -0.015274 -0.041076  0.084756  \n",
              "engType           0.152988 -0.040471 -0.044859  \n",
              "registration      1.000000  0.185104 -0.055793  \n",
              "year              0.185104  1.000000 -0.169709  \n",
              "drive            -0.055793 -0.169709  1.000000  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        }
      ]
    },
    {
      "metadata": {
        "id": "rsXnOy1JrSPw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "fc6f4f6b-d8ec-4359-f19a-489eb2f03836"
      },
      "cell_type": "code",
      "source": [
        "# Standardizing/normalize the data\n",
        "df = (df - df.mean()) / df.std()\n",
        "df.head()"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>make</th>\n",
              "      <th>price</th>\n",
              "      <th>body</th>\n",
              "      <th>mileage</th>\n",
              "      <th>engV</th>\n",
              "      <th>engType</th>\n",
              "      <th>registration</th>\n",
              "      <th>year</th>\n",
              "      <th>drive</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.959604</td>\n",
              "      <td>-0.028035</td>\n",
              "      <td>-1.429724</td>\n",
              "      <td>-0.756630</td>\n",
              "      <td>-0.012685</td>\n",
              "      <td>-0.485072</td>\n",
              "      <td>0.248999</td>\n",
              "      <td>0.505222</td>\n",
              "      <td>0.572196</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.141257</td>\n",
              "      <td>0.176467</td>\n",
              "      <td>0.433274</td>\n",
              "      <td>0.320690</td>\n",
              "      <td>-0.142622</td>\n",
              "      <td>-0.485072</td>\n",
              "      <td>0.248999</td>\n",
              "      <td>0.649607</td>\n",
              "      <td>1.921296</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.141257</td>\n",
              "      <td>0.769522</td>\n",
              "      <td>-0.187725</td>\n",
              "      <td>-0.069197</td>\n",
              "      <td>0.544187</td>\n",
              "      <td>1.006039</td>\n",
              "      <td>0.248999</td>\n",
              "      <td>0.216451</td>\n",
              "      <td>1.921296</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.141257</td>\n",
              "      <td>0.066036</td>\n",
              "      <td>1.675273</td>\n",
              "      <td>0.207828</td>\n",
              "      <td>-0.142622</td>\n",
              "      <td>-1.230627</td>\n",
              "      <td>0.248999</td>\n",
              "      <td>0.793992</td>\n",
              "      <td>-0.776904</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.345120</td>\n",
              "      <td>0.016955</td>\n",
              "      <td>-1.429724</td>\n",
              "      <td>-0.602727</td>\n",
              "      <td>-0.105497</td>\n",
              "      <td>1.006039</td>\n",
              "      <td>0.248999</td>\n",
              "      <td>0.938378</td>\n",
              "      <td>0.572196</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       make     price      body   mileage      engV   engType  registration  \\\n",
              "0 -0.959604 -0.028035 -1.429724 -0.756630 -0.012685 -0.485072      0.248999   \n",
              "1  0.141257  0.176467  0.433274  0.320690 -0.142622 -0.485072      0.248999   \n",
              "2  0.141257  0.769522 -0.187725 -0.069197  0.544187  1.006039      0.248999   \n",
              "3  0.141257  0.066036  1.675273  0.207828 -0.142622 -1.230627      0.248999   \n",
              "4  0.345120  0.016955 -1.429724 -0.602727 -0.105497  1.006039      0.248999   \n",
              "\n",
              "       year     drive  \n",
              "0  0.505222  0.572196  \n",
              "1  0.649607  1.921296  \n",
              "2  0.216451  1.921296  \n",
              "3  0.793992 -0.776904  \n",
              "4  0.938378  0.572196  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "metadata": {
        "id": "xXTBGjPiZ5Mm",
        "colab_type": "code",
        "outputId": "482d2c21-a4c8-4036-b7ba-2198c4d002bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "cell_type": "code",
      "source": [
        "# Sklearn to find the optimal parameters\n",
        "\n",
        "# Creating X matrix\n",
        "X = df[['year', 'drive']]\n",
        "\n",
        "# Adding ones column to X\n",
        "X = np.c_[np.ones(X.shape[0]), X]\n",
        "\n",
        "# Setting y/dependent variable\n",
        "y = df[['price']].values\n",
        "\n",
        "# Fitting model\n",
        "lr = LinearRegression()\n",
        "lr.fit(X, y)\n",
        "\n",
        "# Making coefficient and intercept variables\n",
        "beta_1 = lr.coef_\n",
        "beta_0 = lr.intercept_\n",
        "\n",
        "# Looking at coefs and intercept\n",
        "print('Coefficients:', beta_1)\n",
        "print('Intercept:', beta_0)"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Coefficients: [[0.         0.44401757 0.2973055 ]]\n",
            "Intercept: [-2.83355711e-15]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_tWzF6IqXIIq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "##### Solve by implementing a \"multiple variable\" Gradient Descent Function #####"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qpBC32l0r9Ri",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Setting hyperparameters\n",
        "\n",
        "alpha = 0.02 # size of step\n",
        "iters = 1000 # number of steps\n",
        "theta = np.zeros([1,3]) # starting position of theta -- all zeros"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jwAjgPSC4um7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Making compute cost function\n",
        "def computeCost(X,y,theta):\n",
        "    tobesummed = ((X @ theta.T)-y) ** 2 # Part of the cost function (https://chrisjmccormick.files.wordpress.com/2014/03/gradientdescentofmsetable.png)\n",
        "    return np.sum(tobesummed)/(2 * len(X)) # Summation and (1/2m) of cost function"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BcpWojEU4uwC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "12409061-1630-4e20-fe8d-17d2401fa46e"
      },
      "cell_type": "code",
      "source": [
        "# making gradient descent function\n",
        "def gradientDescent(X,y,theta,iters,alpha):\n",
        "    cost = np.zeros(iters)\n",
        "    for i in range(iters):\n",
        "        theta = theta - (alpha/len(X)) * np.sum(X * (X @ theta.T - y), axis=0) # This is bulk of the gradient descent function -- still having trouble comprehending\n",
        "        cost[i] = computeCost(X, y, theta) # The cost of each iteration\n",
        "    \n",
        "    return theta,cost\n",
        "\n",
        "# running the gd and cost function\n",
        "g,cost = gradientDescent(X,y,theta,iters,alpha)\n",
        "print(g)\n",
        "# print(cost)\n",
        "# finalCost = computeCost(X,y,g)\n",
        "# print(finalCost)\n",
        "\n",
        "# Got the same result as the sklearn LinearRegression model"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-2.82469803e-15  4.44017552e-01  2.97305475e-01]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "y-kO4-jN4utv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "outputId": "84f4e914-1696-4170-e795-84b2a74f9ff3"
      },
      "cell_type": "code",
      "source": [
        "# Plotting the cost against iterations\n",
        "fig, ax = plt.subplots()  \n",
        "ax.plot(np.arange(iters), cost, 'r')  \n",
        "ax.set_xlabel('Iterations')  \n",
        "ax.set_ylabel('Cost')  \n",
        "ax.set_title('Error vs. Training Epoch');"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfUAAAFnCAYAAAC/5tBZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3Xl8VPW9//H3mZmESDIJCcywCUoj\nBg0KpooioBYJxKLYojVRAlpbrUpb1ws0xUZvMSCFXhWscF2wxS38cNxZXPGiDYvFAsa6gBp2kpCN\nhECSmfP7IzAQSELIzGRmktfz8eDRnDNzznzyadr3fL9nM0zTNAUAAMKeJdgFAAAA/yDUAQBoJwh1\nAADaCUIdAIB2glAHAKCdINQBAGgnbMEuAAhXSUlJ6tu3r6xWa4P1s2fP1vnnnx+kqlrub3/7m958\n801J0u7du2W32xUTEyNJevLJJ5WYmNjifd18882aMmWKkpOTm3zPCy+8oOLiYt1zzz2+FX7YyJEj\nZZqmOnXq1GD9/fffr9TUVL98xhETJ07U9ddfr2uvvdav+wX8zeA6daB1kpKS9PHHH6tHjx7BLsVn\n4RhaI0eO1OzZs3XhhRcG/LPCsT/omJh+BwJgx44dGj58uHJycpSZmSmp/kvAwoULNWbMGLndbn31\n1VfKyMhQWlqarr32Wq1evVqStHbtWmVkZOjuu+/W/fff32C/L774ou644w7vstvt1sUXX6ytW7dq\n+fLluvrqq3XVVVfpmmuu0dq1a336HaZNm6aZM2fqmmuu0fLly1VdXa177rlHY8aM0ciRI/Xoo496\n3zty5Eh99tln3t/7H//4h6655hqNGDFCy5YtkyTNmzdPf/zjHyXVh+SiRYt04403asSIEbrvvvt0\nZHzhcrk0bNgwjRs3Ti6XS0lJSadc+44dO5SSkqJnnnlGV199tYYPH673339fkuTxePQ///M/SktL\nU1pamqZNm6YDBw5IkrZv364JEyYoNTVV1113nfLz8xvsc+LEid56PR5P6xoLBBChDgRIWVmZzjnn\nHL3wwgvedaZpauXKlTIMQ/fdd58yMzO1YsUKzZgxQ/fff78qKyslSV9++aUyMjI0d+7cBvscPXq0\n1q5dq+rqaknS+vXr5XQ6lZiYqIcfflgLFy7U8uXLlZ2drQ8//NDn3yEvL09Lly7VVVddpZdffllV\nVVVasWKFXnvtNblcLn322WcnbFNaWiqLxaK33npLWVlZeuyxxxrd94cffqhFixZp5cqVWrNmjTZs\n2KCysjI9/PDDWrRokV5//XV98sknra69qqpKhmHo7bff1uzZszV9+nTV1dVp+fLl+r//+z+5XC69\n8847qqio0PPPPy9JevDBBzV27Fi99957uvPOOzVlyhTv/tatW6enn35aK1as0Nq1a7Vhw4ZW1wYE\nCsfUAR9MnDixwTH1hIQEvfTSS5Kk2traE47tXnHFFZLqR33FxcUaO3asJOm8885Tr169tHnzZlks\nFkVFRWno0KEnfJ7D4dC5556rTz/9VKNGjdL777+vq666SpLUtWtXvfLKK8rIyNCFF17ol2npoUOH\neo9Z33rrrZo4caIMw1BcXJz69++vHTt2nPA5dXV1Gj9+vCQpOTlZu3btanTfaWlpioqKkiSdeeaZ\n2r17tyorK3XmmWfq7LPPliTdeOONeuedd5qs77/+679OOKZ+5DwBSbr++uslSZdeeqnq6upUUFCg\nVatW6Wc/+5k6d+4sSRo/frwWLVqkX/3qV1q7dq2eeOIJSdKVV17Z4L+D0aNHe+s944wztGfPnuZa\nBwQFoQ74YPHixU0eU7dard4Tz47o0qWLJKmkpER2u12GYXhfi42NVUlJibp166a4uLgmP3PMmDH6\n8MMPNWrUKH3wwQdatGiRJOmpp57SU089pfHjx6tnz57KysrSkCFDfPr9jq3jhx9+0KxZs/Tdd9/J\nYrFoz5493vA+/vc+EpgWi6XJaepje2O1WuV2u1VRUdHgM7t3795sfX/5y1+a/PJy5MvHEbGxsSov\nL1dJSUmD9XFxcdq3b5/Kysrk8Xhkt9u920dHRzdbLxBqmH4HgqBr164qLy/XseeplpWVqWvXrifd\ndsyYMfr444+1efNmxcXF6cwzz5Qk9e3bVzNnzlReXp4mTZp0wvF4X/33f/+3+vfvr+XLl2vFihUa\nMGCAX/cv1QfnkePbklRYWNjqfZmmqdLSUu9yeXm54uLi1K1bN5WVlXnXl5WVqVu3boqPj5dhGN5t\nTNNUQUGBOJcY4YRQB4Lg9NNPV48ePbwnkW3YsEHFxcUtuhSue/fu6tOnjxYsWOCdei8pKdEvf/lL\nVVZWymKxaNCgQQ1mAfxh3759Ouecc2S1WvXpp5+qoKCgQQD7Q3Jysr7++msVFBTI4/Fo6dKlPu3v\n7bffliR98sknioqKUr9+/XTFFVfozTffVHV1terq6rR06VJdfvnlioyM1LBhw/Taa69JklavXq3b\nb7/d730EAonpd8AHxx9Tl6TMzEzvsfOmGIahv/71r8rOztb8+fN12mmn6fHHH/dOW5/MmDFjNGvW\nLE2dOlVS/bH8ESNG6LrrrpPValVERIQeeeQRSf67PvzOO+/UzJkz9be//U1XXnmlfvvb3+qJJ57Q\nOeec49N+j+V0OnXfffdp0qRJ6tatmzIyMrwh25jGjqmnpqYqPT1dVqtVtbW1Gjt2rMrLyzVjxgxZ\nLBalpaXp66+/1vjx42Wapi6++GJNmjRJkvTII4/ogQce0EsvvaS4uDjNmTPHb78b0Ba4Th1ASDFN\n0zs6/vbbb3XTTTdp/fr1p7SPHTt2aPTo0fryyy8DUSIQsph+BxAy6urqNGLECG3cuFGStGzZMg0e\nPDjIVQHhg+l3ACHDZrMpOztbU6dOlWmacjgc3sMIAE6O6XcAANoJpt8BAGgnCHUAANqJgB5Tz8nJ\n0caNG2UYhrKyshpcgzty5Ej16NHDeznQnDlz1L1792a3aUxR0X6/1hwf31mlpf699rajoYf+QR99\nRw99Rw995+8eOhz2Jl8LWKivW7dOBQUFys3N1datW5WVlaXc3NwG73n66acb3IaxJdsEms1mPfmb\n0Cx66B/00Xf00Hf00Hdt2cOATb/n5eVp1KhRkqTExESVl5d7n0Dlz20AAEC9gIV6cXGx4uPjvcsJ\nCQkqKipq8J7s7GzdeOONmjNnjkzTbNE2AACgcW12nfrxV879/ve/14gRIxQXF6fJkydr5cqVJ92m\nMfHxnf0+tdHc8Qq0DD30D/roO3roO3rou7bqYcBC3el0qri42LtcWFgoh8PhXf7Zz37m/fmyyy7T\nN998c9JtGuPvEzgcDrvfT77raOihf9BH39FD39FD3/m7h819QQjY9PuwYcO8o+/8/Hw5nU7v84j3\n79+vX/3qV6qpqZEkrV+/Xv379292GwAA0LyAjdRTUlKUnJysjIwMGYah7OxsuVwu2e12paam6rLL\nLlN6ero6deqkc889V2lpaTIM44RtAABAy4T9bWL9PS3EVJPv6KF/0Eff0UPf0UPftYvpdwAA0LYI\ndQAA2glCHQCAdoJQP4b1P19K770X7DIAAGgVQv0YMQ/9URo3TgrvcwcBAB0UoX4M02KRDh6UqquD\nXQoAAKeMUD+GGV1/oxujqirIlQAAcOoI9WOYhx8Da1TxZDgAQPgh1I9xNNQZqQMAwg+hfiym3wEA\nYYxQPwbT7wCAcEaoH4PpdwBAOCPUj3H07HdG6gCA8EOoH4OROgAgnBHqxyDUAQDhjFA/BtPvAIBw\nRqgfg5E6ACCcEerH8Ib6AUIdABB+CPVjMP0OAAhnhPoxmH4HAIQzQv0YZmdCHQAQvgj1Y1mt0mmn\nMf0OAAhLhPrxYmIYqQMAwhKhfjxCHQAQpgj148XEMP0OAAhLhPrxGKkDAMIUoX68mBgZtbVSTU2w\nKwEA4JQQ6seL4QY0AIDwRKgfzxvqTMEDAMILoX487ioHAAhThPrxjozUeagLACDMEOrHY/odABCm\nCPXjEeoAgDBFqB+Ps98BAGGKUD8eI3UAQJgi1I/HSB0AEKYI9eMxUgcAhClC/XiEOgAgTAU01HNy\ncpSenq6MjAxt2rSp0ffMnTtXEydOlCRVVVXpt7/9rSZOnKiMjAytXr06kOU1jul3AECYsgVqx+vW\nrVNBQYFyc3O1detWZWVlKTc3t8F7tmzZovXr1ysiIkKS9Nprr6lfv366//77tXfvXt18881asWJF\noEpsnN0uSTL272/bzwUAwEcBG6nn5eVp1KhRkqTExESVl5ersrLh6HfWrFm69957vcvx8fEqKyuT\nJFVUVCg+Pj5Q5TXtSKhXMlIHAISXgIV6cXFxg1BOSEhQUVGRd9nlcmnIkCHq3bu3d93YsWO1a9cu\npaamKjMzU1OnTg1UeU3zhjojdQBAeAnY9PvxTNP0/lxWViaXy6VFixZp79693vVvvPGGevXqpWef\nfVZfffWVsrKy5HK5mt1vfHxn2WxW/xbbubMiDx6Qw2H37347EHrnH/TRd/TQd/TQd23Vw4CFutPp\nVHFxsXe5sLBQDodDkrRmzRqVlJRowoQJqqmp0bZt25STk6NDhw5p+PDhkqQBAwaosLBQbrdbVmvT\noV1aesCvdTscdrlj7DJLy1RaxGi9NRwOu4ronc/oo+/ooe/ooe/83cPmviAEbPp92LBhWrlypSQp\nPz9fTqdTMYfPLE9LS9OyZcu0ZMkSzZ8/X8nJycrKytIZZ5yhjRs3SpJ27typ6OjoZgM9UMyYGE6U\nAwCEnYCN1FNSUpScnKyMjAwZhqHs7Gy5XC7Z7XalpqY2uk16erqysrKUmZmpuro6PfTQQ4Eqr1mm\nPVbWPbuD8tkAALSWYR57sDsM+XtayOGwq2bE5Yr85P9UtLtUCsJMQbhjus4/6KPv6KHv6KHv2sX0\nezgzYzgDHgAQfgj1RphH7irHcXUAQBgh1Bthclc5AEAYItQbYdpjJTH9DgAIL4R6IzyM1AEAYYhQ\nb4T3mDojdQBAGCHUG3Hk7HcLI3UAQBgh1BvBMXUAQDgi1BvB2e8AgHBEqDeC69QBAOGIUG8E0+8A\ngHBEqDfCO/1OqAMAwgih3ghPNNPvAIDwQ6g3JjpapmFwSRsAIKwQ6o0xDJn2WEbqAICwQqg3wbTb\nZVRWBrsMAABajFBvghkTI6OyIthlAADQYoR6E8wYe/30u2kGuxQAAFqEUG+CabfLqK2VDh0KdikA\nALQIod6Eozeg4bg6ACA8EOpN8HhvFctxdQBAeCDUm3DkrnIW7ioHAAgThHoTjjxT3ahgpA4ACA+E\nehPMuC6SCHUAQPgg1JtgxsVJkoyK8iBXAgBAyxDqTfAcPvvdQqgDAMIEod4E70i9nFAHAIQHQr0J\nhDoAINwQ6k3wxB4Oda5TBwCECUK9CebhULcwUgcAhAlCvQlm7OHbxHKiHAAgTBDqTYmIkNk5mmPq\nAICwQag3wxMXJ0t5WbDLAACgRQj1ZpixsZwoBwAIG4R6M8zYuPrpd9MMdikAAJwUod4MT1ycDLdb\nqqoKdikAAJwUod4M72VtnAEPAAgDhHozjl7WxnF1AEDoC2io5+TkKD09XRkZGdq0aVOj75k7d64m\nTpzoXX7zzTc1btw4jR8/XqtWrQpkeSflffwql7UBAMJAwEJ93bp1KigoUG5urh555BE98sgjJ7xn\ny5YtWr9+vXe5tLRUTz75pF566SUtWLBAH3zwQaDKaxGPd/qdy9oAAKEvYKGel5enUaNGSZISExNV\nXl6uysrKBu+ZNWuW7r333gbbDB06VDExMXI6nfrzn/8cqPJahIe6AADCiS1QOy4uLlZycrJ3OSEh\nQUVFRYqJiZEkuVwuDRkyRL179/a+Z8eOHTp48KDuuOMOVVRU6He/+52GDh3a7OfEx3eWzWb1a+0O\nh73+hz49JEmxnkPSkXVoEQf98gv66Dt66Dt66Lu26mHAQv145jHXepeVlcnlcmnRokXau3dvg/eV\nlZVp/vz52rVrlyZNmqSPPvpIhmE0ud/S0gN+rdPhsKuoaL8kKcKMUBdJVbsKdeDwOpzcsT1E69FH\n39FD39FD3/m7h819QQhYqDudThUXF3uXCwsL5XA4JElr1qxRSUmJJkyYoJqaGm3btk05OTlKSkrS\nBRdcIJvNpr59+yo6OlolJSXq2rVroMpsFtPvAIBwErBj6sOGDdPKlSslSfn5+XI6nd6p97S0NC1b\ntkxLlizR/PnzlZycrKysLA0fPlxr1qyRx+NRaWmpDhw4oPj4+ECVeFLes9+5Th0AEAYCNlJPSUlR\ncnKyMjIyZBiGsrOz5XK5ZLfblZqa2ug23bt315gxY3TDDTdIkqZPny6LJXiX0h85+52ROgAgHBim\nGd43Nvf3sZ4Gxz4OHpSjr1M1V4xU+ZLX/fo57RnH4PyDPvqOHvqOHvquLY+pc0e55kRFyezUiel3\nAEBYINRPwoyNk1HGzWcAAKGPUD8JT5cuPNAFABAWCPWTMLvEyygt5ZnqAICQR6ifhCc+XobbLWM/\nT2oDAIQ2Qv0kzPgESaofrQMAEMII9ZPwdKm/+Y2ljFAHAIQ2Qv0kzMN3tGOkDgAIdYT6STBSBwCE\nC0L9JBipAwDCBaF+EozUAQDhglA/CUbqAIBwQaifBCN1AEC4INRP4uhIvSTIlQAA0DxC/SRMe6xM\ni0UWpt8BACGOUD8Zi0Vmly4ymH4HAIQ4Qr0FPF3iGakDAEIeod4CZnxC/UidJ7UBAEIYod4Cnvh4\nGbW1UlVVsEsBAKBJhHoLmFzWBgAIA4R6C3i4AQ0AIAwQ6i3ASB0AEA4I9RbwjtQJdQBACCPUW8A7\nUi/hrnIAgNBFqLfAkVvFWrhVLAAghBHqLeBJ6CpJMhipAwBCGKHeAkdC3bKvOMiVAADQNEK9BTxd\nu0ki1AEAoY1Qb4noaJlRUTJK9gW7EgAAmkSot4RhyNO1myz7CHUAQOgi1FvIk9CV6XcAQEgj1FvI\n7NpVxoED0oEDwS4FAIBGEeotxMlyAIBQR6i3kKfb4VDnZDkAQIgi1FvIPHIDGkbqAIAQRai3kHf6\nvZhQBwCEJkK9hY4eU2f6HQAQmloU6u+8884J615++eWTbpeTk6P09HRlZGRo06ZNjb5n7ty5mjhx\nYoN1Bw8e1KhRo+RyuVpSXpvwhjrH1AEAIcrW3Itffvml8vPz9dxzz6m6utq7vra2Vk8++aRuvPHG\nJrddt26dCgoKlJubq61btyorK0u5ubkN3rNlyxatX79eERERDdY/9dRTiouLa83vEzBmV46pAwBC\nW7Oh3qlTJ+3bt0/79+/Xv/71L+96wzA0ZcqUZnecl5enUaNGSZISExNVXl6uyspKxcTEeN8za9Ys\n3XvvvZo/f7533datW7VlyxZdccUVrfl9AsZzONQ5pg4ACFXNhnpiYqISExN1ySWXaPDgwd71Ho9H\nFkvzM/fFxcVKTk72LickJKioqMgb6i6XS0OGDFHv3r0bbPfoo4/qwQcf1Ouvv37Kv0wgmV3iZVqt\nXKcOAAhZzYb6Ed99953y8/OVkZGhzMxM7dmzR7fddptuuummFn+QaZren8vKyuRyubRo0SLt3bvX\nu/7111/X4MGD1adPnxbvNz6+s2w2a4vf3xIOh73xF7p2VUR5adOvw4se+Qd99B099B099F1b9bBF\noZ6bm6vFixfrvffeU//+/fXiiy/q5ptvbjbUnU6nio+Zqi4sLJTD4ZAkrVmzRiUlJZowYYJqamq0\nbds25eTkqLCwUNu3b9eqVau0Z88eRUZGqkePHrr00kub/JzSUv/ettXhsKuoaH+jr8XHJ8iyd4/2\nNfE66jXXQ7QcffQdPfQdPfSdv3vY3BeEFoV6p06dFBkZqY8//ljjxo076dS7JA0bNkzz5s1TRkaG\n8vPz5XQ6vVPvaWlpSktLkyTt2LFDf/jDH5SVldVg+3nz5ql3797NBnpb83TtJtvXX0m1tdJxJ/cB\nABBsLQp1SXr44Ye1YcMGzZgxQ59//rlqamqafX9KSoqSk5OVkZEhwzCUnZ0tl8slu92u1NRUnwsP\nBvPwZW1GSYnM7t2DXA0AAA0Z5rEHu5tQWFioZcuW6fLLL1e/fv309ttv66yzztKAAQPaosZm+Xta\nqLlpkphp9+u0555WyUf/lDt5oF8/tz1hus4/6KPv6KHv6KHv2nL6vUU3n3E6nRo4cKBWrVql559/\nXr179w6JQG9rHmf96NxSuPck7wQAoO21KNQff/xxzZ49W4WFhdq7d69mzJihhQsXBrq2kONxOCUR\n6gCA0NSiY+pr167VK6+84j1Brq6uTpmZmfrNb34T0OJCzdGRemGQKwEA4EQtGqkff7MZm80mwzAC\nVlSo8jgPj9SLCHUAQOhp0Uh94MCBuuOOO7yXl/3zn//UwIEd70QxjqkDAELZSUN9+/btysrK0vLl\ny7Vx40YZhqELL7xQv/71r9uivpDi6VZ/8xxG6gCAUNTs9HteXp5uvPFGVVVVaezYscrKytL48eP1\n8ssv64svvmirGkNHp07yxMczUgcAhKRmQ33+/Pl67rnnZLcfvSYuKSlJCxYs0GOPPRbw4kKRx+Fk\npA4ACEnNhrppmjr77LNPWN+/f38dOnQoYEWFMo+zuywlJdJJ7qgHAEBbazbUDxxo+mEpZWVlfi8m\nHHjPgC8uCnIlAAA01Gyo9+/fXy+//PIJ659++mkNGjQoYEWFMo+DM+ABAKGp2bPfp0yZosmTJ+uN\nN97QwIED5fF4tGHDBsXExHTIO8pJXNYGAAhdzYa6w+HQkiVLlJeXp2+//VZWq1VXXXWVLrrooraq\nL+QcvQEN0+8AgNDSopvPDB06VEOHDg10LWGB+78DAEJVi24Ti6OYfgcAhCpC/RTxUBcAQKgi1E+R\n2bWrTKtVlr17gl0KAAANEOqnymqtvwHNnt3BrgQAgAYI9Vbw9OpVH+qmGexSAADwItRbwdOjl4ya\nGhn79gW7FAAAvAj1VnD37ClJsu7eGeRKAAA4ilBvBU+PXpIky+5dQa4EAICjCPVW8PQ6EuqcLAcA\nCB2Eeit4eh4JdabfAQChg1BvBc/hY+qM1AEAoYRQbwX34WPqVo6pAwBCCKHeGp07y9OlCyfKAQBC\nCqHeSp6evZh+BwCEFEK9lTw9espSUS5VVga7FAAAJBHqrebu1VuSZOUe8ACAEEGot5Knx5Ez4Dmu\nDgAIDYR6K3mvVd/FteoAgNBAqLeS+/Q+kiTrju1BrgQAgHqEeit5+vSVJFkIdQBAiCDUW8nd+3RJ\nknU7oQ4ACA2Eemt17ixPt26ybC8IdiUAAEgi1H3i7tNX1p07JI8n2KUAAECo+8Jzel8ZNTWyFBUG\nuxQAAAIb6jk5OUpPT1dGRoY2bdrU6Hvmzp2riRMnepdnz56t9PR0XXfddXr33XcDWZ7PjpwBb9nG\nFDwAIPhsgdrxunXrVFBQoNzcXG3dulVZWVnKzc1t8J4tW7Zo/fr1ioiIkCStWbNG3377rXJzc1Va\nWqqf//znGj16dKBK9Jm7z9HL2uouujjI1QAAOrqAjdTz8vI0atQoSVJiYqLKy8tVedx90mfNmqV7\n773Xu3zRRRfp8ccflyTFxsaqurpabrc7UCX6zNPnDEmShTPgAQAhIGChXlxcrPj4eO9yQkKCioqK\nvMsul0tDhgxR7969veusVqs6d+4sSVq6dKkuu+wyWa3WQJXoM+8NaLZvC3IlAAAEcPr9eKZpen8u\nKyuTy+XSokWLtHfv3hPe+/7772vp0qV67rnnTrrf+PjOstn8G/wOh71lb7zgXEnSaYW7dFpLt+kg\nWtxDNIs++o4e+o4e+q6tehiwUHc6nSouLvYuFxYWyuFwSKo/dl5SUqIJEyaopqZG27ZtU05OjrKy\nsrR69WotWLBAzzzzjOz2kzehtPSAX+t2OOwqKtrfwndb1DU2Tp7vvldpi7dp/06th2gKffQdPfQd\nPfSdv3vY3BeEgE2/Dxs2TCtXrpQk5efny+l0KiYmRpKUlpamZcuWacmSJZo/f76Sk5OVlZWl/fv3\na/bs2Vq4cKG6dOkSqNL8ytOnb/30+zEzEQAABEPARuopKSlKTk5WRkaGDMNQdna2XC6X7Ha7UlNT\nG91m2bJlKi0t1T333ONd9+ijj6pXr16BKtNn7jPOlC1/s4yiIplOZ7DLAQB0YAE9pv7AAw80WB4w\nYMAJ7zn99NO1ePFiSVJ6errS09MDWZLfufv9SJJk/f471RHqAIAg4o5yPjoa6luDXAkAoKMj1H3k\nDfUfvgtyJQCAjo5Q95H7zH6SJOsP3we5EgBAR0eo+8jTq7fMyEhZv2ekDgAILkLdV1ar3GecSagD\nAIKOUPcDd78fyVJWJqO0JNilAAA6MELdD469rA0AgGAh1P3AfSahDgAIPkLdD45e1sYZ8ACA4CHU\n/cAb6lu3BLkSAEBHRqj7gafvGTI7dZL122+CXQoAoAMj1P3BapX7R2fJ9u03kscT7GoAAB0Uoe4n\ndWcnyThQJcuuncEuBQDQQRHqfuLuf7YkyfrN10GuBADQURHqfuJOqn+srO1bQh0AEByEup/U9U+S\nJFm/4WQ5AEBwEOp+4k48S6bFIisjdQBAkBDq/tKpk9xnnMn0OwAgaAh1P3KfnSTLvn0y9u0LdikA\ngA6IUPcj99mHT5b7+j9BrgQA0BER6n5Ud865kiTrl18EuRIAQEdEqPtRXfJ5kiRbPqEOAGh7hLof\nuc/qLzMyUrb8zcEuBQDQARHq/hQRobqkc2T76j9SXV2wqwEAdDCEup+5kwfKOHhQ1u+2BrsUAEAH\nQ6j7Wd3AI8fVmYIHALQtQt3PvCfLfUGoAwDaFqHuZ3XJAyUxUgcAtD1C3c/MLvFyn95Hts2bJNMM\ndjkAgA6EUA+AusEpshQVyrJrZ7BLAQB0IIR6ANRe8GNJkm3Dv4JcCQCgIyHUA6AupT7UIz4n1AEA\nbYdQD4C6QYNlGoZshDoAoA0R6gFgxtjlThog278/l9zuYJcDAOggCPUAqb3gx7JUVcr67TfBLgUA\n0EEQ6gFSNzhFkhSx4bMgVwIA6CgI9QCpu/AiSZJt/dogVwIA6CgI9QCpO3egPPZYReR9GuxSAAAd\nREBDPScnR+np6crIyNCmTZvZVaLMAAATzUlEQVQafc/cuXM1ceLEU9omLFitqh1ysWzfbZWxd2+w\nqwEAdAABC/V169apoKBAubm5euSRR/TII4+c8J4tW7Zo/fr1p7RNOKkdOkySFLH2n0GuBADQEQQs\n1PPy8jRq1ChJUmJiosrLy1VZWdngPbNmzdK99957StuEk9pL6kM9kil4AEAbCFioFxcXKz4+3ruc\nkJCgoqIi77LL5dKQIUPUu3fvFm8TbuoGXyAzKkoReYzUAQCBZ2urDzKPeWJZWVmZXC6XFi1apL3N\nHG82W/CUs/j4zrLZrH6p8QiHw+6/nQ0dKtuqVXJYa6WEBP/tN8T5tYcdGH30HT30HT30XVv1MGCh\n7nQ6VVxc7F0uLCyUw+GQJK1Zs0YlJSWaMGGCampqtG3bNuXk5DS7TVNKSw/4tW6Hw66iov1+21/n\nIZcq+qOPVP7a26oZ93O/7TeU+buHHRV99B099B099J2/e9jcF4SATb8PGzZMK1eulCTl5+fL6XQq\nJiZGkpSWlqZly5ZpyZIlmj9/vpKTk5WVldXsNuGq5idXSpIiP/ogyJUAANq7gI3UU1JSlJycrIyM\nDBmGoezsbLlcLtntdqWmprZ4m3BXN+gCeeLj60PdNCXDCHZJAIB2yjBbcuA6hPl7WigQU032229R\n1OsulaxeJ3fSAL/uOxQxXecf9NF39NB39NB37WL6HUfV/KT+Mr3Ij94PciUAgPaMUG8DtUeOq39I\nqAMAAodQbwOeHj1Ve/5gRXy6WkZFebDLAQC0U4R6G6n56dUyamsV+d7KYJcCAGinCPU2cuin10iS\nOi17O8iVAADaK0K9jbiTBqjuR4mK/OA9qbo62OUAANohQr2tGIZqfnqNjANVilz1YbCrAQC0Q4R6\nGzp0bf1tYju9uiTIlQAA2iNCvQ3VnT9YdWcnqdPKZTLKy4JdDgCgnSHU25Jh6ND16TIOHVKnt94I\ndjUAgHaGUG9jB6+7QZLU6f+9EuRKAADtDaHexjx9+qpm2AhF5n0q65Zvg10OAKAdIdSD4ODNt0qS\nop5/JsiVAADaE0I9CA799Bq5nd0V9cpLUlVVsMsBALQThHowREbq4MRbZKkoV5Tr/wW7GgBAO0Go\nB8nBSb+UGRGh056aJ7ndwS4HANAOEOpB4unZSwd/kSHblm8Vyf3gAQB+QKgHUfXv7pFpGOr8+FzJ\nNINdDgAgzBHqQeRO7K9D436uiE3/VuS7K4JdDgAgzBHqQXbggWkyLRZFz8iW6uqCXQ4AIIwR6kHm\nThqggzdmyvb1V4rKfSnY5QAAwhihHgIOTMmSedpp6jzzzzzoBQDQaoR6CPD07KUD9zwga+FeRef8\nd7DLAQCEKUI9RByYfLfqzk5S1PPPyrZ+bbDLAQCEIUI9VERGqnLO45Kk2Ltuk7G/IsgFAQDCDaEe\nQmovuVTVv7tX1oIfFPNf93LtOgDglBDqIaZq6h9Vm/JjRbn+n6Je/EewywEAhBFCPdRERKhiwXPy\ndOmimCn3KmL1x8GuCAAQJgj1EOQ5s58qnn9JMgzF/jJT1q/+E+ySAABhgFAPUbWXDtf+x56UpaJc\nXcZfTbADAE6KUA9hh36Rof2z5spSXKQuP/+prJs3BbskAEAII9RD3MFbb9P+uU/IKClR/DVjeEwr\nAKBJhHoYODjxFlU8u1iSqbhbblLnWX+WamuDXRYAIMQQ6mGi5upxKn3rXbn79FX0X/+iLlenyvrN\n18EuCwAQQgj1MOI+73yVfvSpDv4iQxGfb1D8FUMV/eAfeAgMAEASoR52zNg47X/yf1X+j1fk6X26\nOi98UgkXD1bnv86WUVYa7PIAAEFEqIepmrSfqmT1OlX+MVvyeBQ9a4YSUgYqZtr9sm38nFvMAkAH\nRKiHs6goVd99v0o25Ksye4bMmBid9tzTik+9XPE/GabOf5kp2+aNBDwAdBCGaQbu//FzcnK0ceNG\nGYahrKwsnX/++d7XlixZoqVLl8pisWjAgAHKzs7WgQMHNHXqVJWXl6u2tlaTJ0/WiBEjmv2MoqL9\nfq3Z4bD7fZ9tpq5OkR+9r6iXXlDku8tlHD5D3t2jp2qHXqraIUNVO+QSuQecI0VEBKyMsO5hCKGP\nvqOHvqOHvvN3Dx0Oe5Ov2fz2KcdZt26dCgoKlJubq61btyorK0u5ubmSpOrqar3zzjt68cUXFRER\noUmTJunzzz/Xl19+qX79+un+++/X3r17dfPNN2vFihWBKrH9sdlUk5qmmtQ0GRXlivzoA0WuXK7I\nVR8o6rVXFfXaq5IkMyJC7rPOVt0556runHPlObOf3Kf3kfv0vjKdTskwgvt7AABaJWChnpeXp1Gj\nRkmSEhMTVV5ersrKSsXExOi0007T3//+d0n1AV9ZWSmHw6H4+Hh9/XX9ZVoVFRWKj48PVHntnhkb\np0PXjteha8dLpinr91tlW7tGEevXyvblF7J99ZVs/8k/cbtOneTp0VOebt3k6dpNZkJXeboe/jk2\nVmZ0tMzomPr/jIk5+vNpp0mRkTIjIiVPdBB+YwBAwEK9uLhYycnJ3uWEhAQVFRUpJibGu+5///d/\n9Y9//EOTJk1Snz591KdPH7lcLqWmpqqiokILFy486efEx3eWzWb1a+3NTW2ELecF0sUXSLqzftnj\nkX74QcrPl77/XiookAoKZBQUyLpzp6ybNvp0gxuHzSZFRp74LyJCslhO/Z/VevTn42cSfFluy321\ngsPnPYAe+o4e+ig9XY7rr2+TjwpYqB+vsUP3t99+uyZNmqTbbrtNP/7xj7Vjxw716tVLzz77rL76\n6itlZWXJ5XI1u9/S0gN+rbNDHT+yO6RLrqj/dzzTlLG/QkZxsSwl+2TZt69+uarq8L/Koz9X7pdR\nXS3V1cqoqVGkPKqtqpZqa2TU1B7+zxrp4CEZFfslmfVfKjweyVP/s+HxSKbnmPX1/wxO8gMQ7iIj\nVXT5GL/tLijH1J1Op4qLi73LhYWFcjjqv++VlZXp22+/1UUXXaSoqChddtll2rBhg3bs2KHhw4dL\nkgYMGKDCwkK53W5Zrf4diaMFDENmbJzM2Dh5fpR4Sps6HHaV+euLkWk2DHq3+8TXm1k21MzrJ9n2\nlJaP/+7hhy8j3brFqLi40uf9dGT00Hf00EemqW7n9JPaqIcBC/Vhw4Zp3rx5ysjIUH5+vpxOp3fq\nva6uTtOmTdObb76p6Ohobd68WePGjZPVatXGjRs1ZswY7dy5U9HR0QR6R2cY9VPvrfw7COtxvsMu\nU1HBriK80UPf0UPfteHJxwEL9ZSUFCUnJysjI0OGYSg7O1sul0t2u12pqamaPHmyJk2aJJvNpqSk\nJF155ZU6cOCAsrKylJmZqbq6Oj300EOBKg8AgHYnoNeptwWuUw899NA/6KPv6KHv6KHv2vI6de4o\nBwBAO0GoAwDQThDqAAC0E4Q6AADtBKEOAEA7QagDANBOEOoAALQThDoAAO0EoQ4AQDsR9neUAwAA\n9RipAwDQThDqAAC0E4Q6AADtBKEOAEA7QagDANBOEOoAALQTtmAXEEpycnK0ceNGGYahrKwsnX/+\n+cEuKWTNnj1b//rXv1RXV6ff/OY3Ou+88zRlyhS53W45HA795S9/UWRkpN588039/e9/l8Vi0Q03\n3KBf/OIXwS49pBw8eFBXX3217rrrLg0dOpQetsKbb76pZ555RjabTb///e+VlJREH09BVVWVpk6d\nqvLyctXW1mry5MlyOBx66KGHJElJSUl6+OGHJUnPPPOMVqxYIcMw9Nvf/laXX355ECsPDd98843u\nuusu3XLLLcrMzNTu3btb/PdXW1uradOmadeuXbJarZo5c6b69OnjW0EmTNM0zbVr15q33367aZqm\nuWXLFvOGG24IckWhKy8vz/z1r39tmqZplpSUmJdffrk5bdo0c9myZaZpmubcuXPNF1980ayqqjJH\njx5tVlRUmNXV1ebYsWPN0tLSYJYecv7617+a48ePN1999VV62AolJSXm6NGjzf3795t79+41p0+f\nTh9P0eLFi805c+aYpmmae/bsMceMGWNmZmaaGzduNE3TNO+77z5z1apV5rZt28yf//zn5qFDh8x9\n+/aZY8aMMevq6oJZetBVVVWZmZmZ5vTp083Fixebpmme0t+fy+UyH3roIdM0TXP16tXm3Xff7XNN\nTL8flpeXp1GjRkmSEhMTVV5ersrKyiBXFZouuugiPf7445Kk2NhYVVdXa+3atbryyislST/5yU+U\nl5enjRs36rzzzpPdbldUVJRSUlK0YcOGYJYeUrZu3aotW7boiiuukCR62Ap5eXkaOnSoYmJi5HQ6\n9ec//5k+nqL4+HiVlZVJkioqKtSlSxft3LnTO1N5pIdr167ViBEjFBkZqYSEBPXu3VtbtmwJZulB\nFxkZqaefflpOp9O77lT+/vLy8pSamipJuvTSS/3yN0moH1ZcXKz4+HjvckJCgoqKioJYUeiyWq3q\n3LmzJGnp0qW67LLLVF1drcjISElS165dVVRUpOLiYiUkJHi3o6cNPfroo5o2bZp3mR6euh07dujg\nwYO64447dNNNNykvL48+nqKxY8dq165dSk1NVWZmpqZMmaLY2Fjv6/SwaTabTVFRUQ3Wncrf37Hr\nLRaLDMNQTU2NbzX5tHU7ZnL33JN6//33tXTpUj333HMaPXq0d31TvaOnR73++usaPHhwk8fP6GHL\nlZWVaf78+dq1a5cmTZrUoEf08eTeeOMN9erVS88++6y++uorTZ48WXa73fs6PWy9U+2dP3pKqB/m\ndDpVXFzsXS4sLJTD4QhiRaFt9erVWrBggZ555hnZ7XZ17txZBw8eVFRUlPbu3Sun09loTwcPHhzE\nqkPHqlWrtH37dq1atUp79uxRZGQkPWyFrl276oILLpDNZlPfvn0VHR0tq9VKH0/Bhg0bNHz4cEnS\ngAEDdOjQIdXV1XlfP7aH33///Qnr0dCp/O/Y6XSqqKhIAwYMUG1trUzT9I7yW4vp98OGDRumlStX\nSpLy8/PldDoVExMT5KpC0/79+zV79mwtXLhQXbp0kVR/POhI/959912NGDFCgwYN0ubNm1VRUaGq\nqipt2LBBF154YTBLDxmPPfaYXn31VS1ZskS/+MUvdNddd9HDVhg+fLjWrFkjj8ej0tJSHThwgD6e\nojPOOEMbN26UJO3cuVPR0dFKTEzUZ599JuloDy+55BKtWrVKNTU12rt3rwoLC3XWWWcFs/SQdCp/\nf8OGDdOKFSskSR999JEuvvhinz+fp7QdY86cOfrss89kGIays7M1YMCAYJcUknJzczVv3jz169fP\nu27WrFmaPn26Dh06pF69emnmzJmKiIjQihUr9Oyzz8owDGVmZmrcuHFBrDw0zZs3T71799bw4cM1\ndepUeniKXnnlFS1dulSSdOedd+q8886jj6egqqpKWVlZ2rdvn+rq6nT33XfL4XDoT3/6kzwejwYN\nGqQ//OEPkqTFixfrrbfekmEYuueeezR06NAgVx9cX3zxhR599FHt3LlTNptN3bt315w5czRt2rQW\n/f253W5Nnz5dP/zwgyIjIzVr1iz17NnTp5oIdQAA2gmm3wEAaCcIdQAA2glCHQCAdoJQBwCgnSDU\nAQBoJwh1oJ1LSkry3kzkjTfe8Nt+33rrLXk8HknSxIkT5Xa7/bZvAK1DqAMdhNvt1t/+9je/7W/e\nvHneUF+8eLGsVqvf9g2gdbhNLNBBZGVlaefOnbr11lv13HPPadmyZXrhhRdkmqYSEhI0Y8YMxcfH\nKyUlRddff708Ho+ysrKUnZ2t7777TjU1NRo0aJCmT5+uJ554QgUFBbrllls0f/58XXzxxcrPz1dN\nTY0efPBB7dmzR3V1dbr22mt10003yeVy6Z///Kc8Ho++//579e7dW/PmzVNhYaEeeOABSfXPlk9P\nT9f1118f5E4BYcznh7cCCGlnn322WVtba27fvt0cMWKEaZqmuWvXLvOaa64xDx06ZJqmaT7//PPm\nzJkzTdM0zaSkJPOTTz4xTbP+eeVHnhNtmqY5ZswY8+uvv26w32N/XrBggff50NXV1eZPfvITc9u2\nbearr75qjhw50qyurjY9Ho955ZVXmvn5+eaiRYvMP/3pT6ZpmubBgwcbfBaAU8dIHeiAPv/8cxUV\nFelXv/qVJKmmpkann366pPonRaWkpEiSYmNjtXv3bqWnpysyMlJFRUUqLS1tcr8bN27U+PHjJUlR\nUVEaOHCg8vPzJUnnn3++9zGVPXv2VHl5uUaMGKGXXnpJ06ZN0+WXX6709PSA/c5AR0CoAx1QZGSk\nzj//fC1cuLDR1yMiIiRJ77zzjjZv3qwXX3xRNpvNG9hNMQyjwbJpmt51xx9zN01TiYmJeuedd7R+\n/XqtWLFCf//73/XKK6+09tcCOjxOlAM6CIvF4j0L/rzzztOmTZtUVFQkSVq+fLnef//9E7bZt2+f\n+vXrJ5vNpi+++ELbtm1TTU2NpPoAP/YRnZI0aNAgrV69WpJ04MAB5efnKzk5ucma3nrrLW3evFmX\nXnqpsrOztXv37hP2CaDlCHWgg3A6nerWrZvGjx8vu92uP/7xj/rNb36jCRMmaOnSpY0+XzwtLU3/\n/ve/lZmZqXfffVe33nqrZsyY4Z06v+6667Rt2zbv+ydOnKiqqipNmDBBN998s+666y7vtH5jzjrr\nLM2aNUuZmZmaNGmSbrvtNtlsTCACrcVT2gAAaCcYqQMA0E4Q6gAAtBOEOgAA7QShDgBAO0GoAwDQ\nThDqAAC0E4Q6AADtBKEOAEA78f8BLE7XifiBppUAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "RCs6EmWhYPM-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Stretch Goals\n",
        "\n",
        "If you happen upon the most useful resources for accomplishing this challenge first, I want you to spend time today studying other variations of Gradient Descent-Based Optimizers. A good list of the most common optimizers can be found in the Keras Documentation: <https://keras.io/optimizers/>\n",
        "\n",
        "- Try and write a function that can perform gradient descent for arbitarily large (in dimensionality) multiple regression models. \n",
        "- Create a notebook for yourself exploring the different gradient descent based optimizers.\n",
        "- How do the above differ from the \"vanilla\" gradient descent we explored today?\n",
        "- How do these different gradient descent-based optimizers seek to overcome the challenge of finding the global minimum among various local minima?\n",
        "- Write a blog post that reteaches what you have learned about these other gradient descent-based optimizers.\n",
        "\n",
        "[Overview of GD-based optimizers](http://ruder.io/optimizing-gradient-descent/)\n",
        "\n",
        "[Siraj Raval - Evolution of Gradient Descent-Based Optimizers](https://youtu.be/nhqo0u1a6fw)"
      ]
    }
  ]
}